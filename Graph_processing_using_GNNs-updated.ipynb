{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf55bac",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102d1416",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7237550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import networkx as nx\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "# featuretools for automated feature engineering\n",
    "import featuretools as ft\n",
    "\n",
    "# matplotlit and seaborn for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 22\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings from pandas\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# modeling \n",
    "import lightgbm as lgb\n",
    "\n",
    "# utilities\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import f1_score, accuracy_score, auc, roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# memory management\n",
    "import gc\n",
    "\n",
    "# import EarlyStopping\n",
    "from pytorchtools import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04035844",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d73fb2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  103\n",
      "Number of columns:  7797\n"
     ]
    }
   ],
   "source": [
    "# read a .csv file as pd.DataFrame\n",
    "df = pd.read_csv('./data/Supplemental Data - Processed.csv')\n",
    "df_columns = df.columns.tolist()\n",
    "print('Number of rows: ', df.shape[0])\n",
    "print('Number of columns: ', df.shape[1])\n",
    "# print('Columns: ', df_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a6c1ceb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categorical features:  30\n",
      "Number of numerical features:  7767\n"
     ]
    }
   ],
   "source": [
    "# count categorical features and numerical features\n",
    "categorical_features = [col for col in df_columns if df[col].dtype == 'object']\n",
    "numerical_features = [col for col in df_columns if df[col].dtype != 'object']\n",
    "print('Number of categorical features: ', len(categorical_features))\n",
    "print('Number of numerical features: ', len(numerical_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df743398",
   "metadata": {},
   "source": [
    "## Visualization of Original Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a2e373",
   "metadata": {},
   "source": [
    "### Remove feature having low variance (<0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d1677532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year_of_Diagnosis  Tumor_Sequence_Number  Age_At_Diagnosis  \\\n",
      "0               2008                      0                64   \n",
      "1               1998                      0                69   \n",
      "2               1998                      0                66   \n",
      "3               2004                      0                72   \n",
      "4               2009                      0                52   \n",
      "\n",
      "   Regional_Lymph_Nodes_examined  Regional_Lymph_Nodes_positive  \\\n",
      "0                             11                              0   \n",
      "1                             12                              0   \n",
      "2                             10                              0   \n",
      "3                             25                              0   \n",
      "4                             18                              0   \n",
      "\n",
      "   Year_of_Last_Contact_or_Death__S  Year_of_Last_Contact_or_Death__F  \\\n",
      "0                              2015                              2015   \n",
      "1                              1999                              1999   \n",
      "2                              2015                              2015   \n",
      "3                              2012                              2012   \n",
      "4                              2015                              2015   \n",
      "\n",
      "   Overall_Survival_1__Days_  Overall_Survival_1__Months_  \\\n",
      "0                       2740                           90   \n",
      "1                        459                           15   \n",
      "2                       6230                          204   \n",
      "3                       2825                           92   \n",
      "4                       2537                           83   \n",
      "\n",
      "   Overall_Survival_2__Days_  ...       8306       8307       8308       8309  \\\n",
      "0                       2599  ...  16.627917  16.734554  16.697517  16.994888   \n",
      "1                        428  ...  16.830417   0.000000  16.642156  17.010478   \n",
      "2                       6170  ...  16.639782   0.000000  16.225933  16.298286   \n",
      "3                       2825  ...  16.702006   0.000000  16.240624  17.246538   \n",
      "4                       2516  ...  16.733236   0.000000  15.757718   0.000000   \n",
      "\n",
      "        8310  8311       8312       8314       8315       8316  \n",
      "0  17.356033   0.0  16.540088  16.824544  16.601595  17.183535  \n",
      "1  17.284266   0.0  18.409225  16.649383  16.570425  17.183928  \n",
      "2   0.000000   0.0  16.768196  16.684280  16.369291  17.068271  \n",
      "3  17.321641   0.0  17.038372  16.672531  16.549712  17.190188  \n",
      "4  17.170831   0.0  16.161420  16.750601  16.513946  16.624117  \n",
      "\n",
      "[5 rows x 7767 columns]\n"
     ]
    }
   ],
   "source": [
    "# select the columns having numerical values\n",
    "df_numerical_features = df[numerical_features]\n",
    "print(df_numerical_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fe297cc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sample_name Gender__Cancer_Registry_ Ethnicity__Cancer_Registry_  \\\n",
      "0      SCC001                   FEMALE                 NON-SPANISH   \n",
      "1      SCC002                     MALE                 NON-SPANISH   \n",
      "2      SCC003                     MALE                 NON-SPANISH   \n",
      "3      SCC004                     MALE                 NON-SPANISH   \n",
      "4      SCC005                     MALE                 NON-SPANISH   \n",
      "\n",
      "  Race__Cancer_Registry_1_ Grade_Differentiation  \\\n",
      "0                    WHITE  MODERATELY DIFFEREN.   \n",
      "1                    WHITE      POORLY DIFFEREN.   \n",
      "2                    WHITE      POORLY DIFFEREN.   \n",
      "3                    WHITE      POORLY DIFFEREN.   \n",
      "4                    WHITE      POORLY DIFFEREN.   \n",
      "\n",
      "                           Histology Laterality  \\\n",
      "0  80703 SQUAMOUS CELL CARCINOMA NOS      RIGHT   \n",
      "1  80703 SQUAMOUS CELL CARCINOMA NOS       LEFT   \n",
      "2  80703 SQUAMOUS CELL CARCINOMA NOS      RIGHT   \n",
      "3  80703 SQUAMOUS CELL CARCINOMA NOS      RIGHT   \n",
      "4  80703 SQUAMOUS CELL CARCINOMA NOS       LEFT   \n",
      "\n",
      "  Summary_of_Treatment__1st_course Surgery_Radiation_Sequence  \\\n",
      "0                          SURGERY                   NOT APPL   \n",
      "1                          SURGERY                   NOT APPL   \n",
      "2                          SURGERY                   NOT APPL   \n",
      "3                        SURG/CHEM                   NOT APPL   \n",
      "4                          SURGERY                   NOT APPL   \n",
      "\n",
      "  Pathological_TNM__T  ... subtype.1  \\\n",
      "0                 P2A  ...  Inflamed   \n",
      "1                 P1B  ...  Inflamed   \n",
      "2                 P1B  ...  Inflamed   \n",
      "3                 P2A  ...  Inflamed   \n",
      "4                 P2B  ...  Inflamed   \n",
      "\n",
      "  Tobacco use (FCDS) [Data element retired in 2011]  \\\n",
      "0                                MODERATE (1-2 PPD)   \n",
      "1                                              HIST   \n",
      "2                                              HIST   \n",
      "3                                              NONE   \n",
      "4                                              HIST   \n",
      "\n",
      "  Recently Reported Cigarette Smoking Status Recently Reported Tobacco Use  \\\n",
      "0                                       EVER                          EVER   \n",
      "1                                       EVER                          EVER   \n",
      "2                                       EVER                          EVER   \n",
      "3                                       EVER                          EVER   \n",
      "4                                       EVER                          EVER   \n",
      "\n",
      "  Trypsin.lot.number Digestion.Start.Time Digestion.End.Time TMT.Sample.Name  \\\n",
      "0           R5E15825              2:34 PM           11:35 AM   TMT26_TMT-130   \n",
      "1           R5E15825              1:00 PM           11:25 AM   TMT23_TMT-131   \n",
      "2           R5E15825              1:00 PM           11:25 AM   TMT24_TMT-128   \n",
      "3           R5E15825              1:00 PM           11:25 AM   TMT23_TMT-127   \n",
      "4           R5E15825              1:54 PM           11:12 AM   TMT10_TMT-127   \n",
      "\n",
      "  Proteomic Subtype Wilkerson Classification  \n",
      "0        Inflamed A                classical  \n",
      "1        Inflamed A                secretory  \n",
      "2        Inflamed A                secretory  \n",
      "3        Inflamed A                primitive  \n",
      "4        Inflamed A                classical  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# select the columns having categorical values\n",
    "df_categorical_features = df[categorical_features]\n",
    "print(df_categorical_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a8141ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes data and returns it after removing the features\n",
    "# having less than the given threshold variance\n",
    "\n",
    "def variance_threshold_selector(data, threshold=0.9):\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(data)\n",
    "    return data[data.columns[selector.get_support(indices=True)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b5337400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(103, 4477)\n"
     ]
    }
   ],
   "source": [
    "x = variance_threshold_selector(df_numerical_features, 0.25)\n",
    "print(type(x))\n",
    "print(x.shape)\n",
    "x_columns = x.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf06e184",
   "metadata": {},
   "source": [
    "## Visualization after removal of low variance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "23bb6f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ???????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fb2379",
   "metadata": {},
   "source": [
    "### Removing colinear features, high correlations (>75%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "52709286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4477, 4477)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_of_Diagnosis</th>\n",
       "      <th>Tumor_Sequence_Number</th>\n",
       "      <th>Age_At_Diagnosis</th>\n",
       "      <th>Regional_Lymph_Nodes_examined</th>\n",
       "      <th>Regional_Lymph_Nodes_positive</th>\n",
       "      <th>Year_of_Last_Contact_or_Death__S</th>\n",
       "      <th>Year_of_Last_Contact_or_Death__F</th>\n",
       "      <th>Overall_Survival_1__Days_</th>\n",
       "      <th>Overall_Survival_1__Months_</th>\n",
       "      <th>Overall_Survival_2__Days_</th>\n",
       "      <th>...</th>\n",
       "      <th>8300</th>\n",
       "      <th>8301</th>\n",
       "      <th>8304</th>\n",
       "      <th>8305</th>\n",
       "      <th>8307</th>\n",
       "      <th>8309</th>\n",
       "      <th>8310</th>\n",
       "      <th>8311</th>\n",
       "      <th>8314</th>\n",
       "      <th>8316</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Year_of_Diagnosis</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100609</td>\n",
       "      <td>0.074038</td>\n",
       "      <td>0.052394</td>\n",
       "      <td>0.116608</td>\n",
       "      <td>0.662213</td>\n",
       "      <td>0.656449</td>\n",
       "      <td>0.284000</td>\n",
       "      <td>0.283572</td>\n",
       "      <td>0.285061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096738</td>\n",
       "      <td>0.024911</td>\n",
       "      <td>0.015518</td>\n",
       "      <td>0.042033</td>\n",
       "      <td>0.051857</td>\n",
       "      <td>0.170086</td>\n",
       "      <td>0.005412</td>\n",
       "      <td>0.049978</td>\n",
       "      <td>0.072603</td>\n",
       "      <td>0.090380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tumor_Sequence_Number</th>\n",
       "      <td>0.100609</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.265371</td>\n",
       "      <td>0.197846</td>\n",
       "      <td>0.096182</td>\n",
       "      <td>0.114551</td>\n",
       "      <td>0.127843</td>\n",
       "      <td>0.044502</td>\n",
       "      <td>0.044990</td>\n",
       "      <td>0.046955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020590</td>\n",
       "      <td>0.039268</td>\n",
       "      <td>0.005099</td>\n",
       "      <td>0.044364</td>\n",
       "      <td>0.073432</td>\n",
       "      <td>0.068132</td>\n",
       "      <td>0.060961</td>\n",
       "      <td>0.075107</td>\n",
       "      <td>0.110043</td>\n",
       "      <td>0.062217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age_At_Diagnosis</th>\n",
       "      <td>0.074038</td>\n",
       "      <td>0.265371</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024481</td>\n",
       "      <td>0.083128</td>\n",
       "      <td>0.089446</td>\n",
       "      <td>0.082757</td>\n",
       "      <td>0.204286</td>\n",
       "      <td>0.204993</td>\n",
       "      <td>0.202093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105379</td>\n",
       "      <td>0.011552</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.017555</td>\n",
       "      <td>0.027586</td>\n",
       "      <td>0.030560</td>\n",
       "      <td>0.007127</td>\n",
       "      <td>0.134809</td>\n",
       "      <td>0.016448</td>\n",
       "      <td>0.107989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Regional_Lymph_Nodes_examined</th>\n",
       "      <td>0.052394</td>\n",
       "      <td>0.197846</td>\n",
       "      <td>0.024481</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.214147</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.105154</td>\n",
       "      <td>0.204488</td>\n",
       "      <td>0.203869</td>\n",
       "      <td>0.202699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162477</td>\n",
       "      <td>0.144585</td>\n",
       "      <td>0.068971</td>\n",
       "      <td>0.095920</td>\n",
       "      <td>0.075105</td>\n",
       "      <td>0.026212</td>\n",
       "      <td>0.079932</td>\n",
       "      <td>0.013017</td>\n",
       "      <td>0.076088</td>\n",
       "      <td>0.032443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Regional_Lymph_Nodes_positive</th>\n",
       "      <td>0.116608</td>\n",
       "      <td>0.096182</td>\n",
       "      <td>0.083128</td>\n",
       "      <td>0.214147</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014539</td>\n",
       "      <td>0.015377</td>\n",
       "      <td>0.165666</td>\n",
       "      <td>0.164628</td>\n",
       "      <td>0.164375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126528</td>\n",
       "      <td>0.052517</td>\n",
       "      <td>0.043171</td>\n",
       "      <td>0.004640</td>\n",
       "      <td>0.122362</td>\n",
       "      <td>0.027579</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.109744</td>\n",
       "      <td>0.073299</td>\n",
       "      <td>0.080292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4477 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Year_of_Diagnosis  Tumor_Sequence_Number  \\\n",
       "Year_of_Diagnosis                       1.000000               0.100609   \n",
       "Tumor_Sequence_Number                   0.100609               1.000000   \n",
       "Age_At_Diagnosis                        0.074038               0.265371   \n",
       "Regional_Lymph_Nodes_examined           0.052394               0.197846   \n",
       "Regional_Lymph_Nodes_positive           0.116608               0.096182   \n",
       "\n",
       "                               Age_At_Diagnosis  \\\n",
       "Year_of_Diagnosis                      0.074038   \n",
       "Tumor_Sequence_Number                  0.265371   \n",
       "Age_At_Diagnosis                       1.000000   \n",
       "Regional_Lymph_Nodes_examined          0.024481   \n",
       "Regional_Lymph_Nodes_positive          0.083128   \n",
       "\n",
       "                               Regional_Lymph_Nodes_examined  \\\n",
       "Year_of_Diagnosis                                   0.052394   \n",
       "Tumor_Sequence_Number                               0.197846   \n",
       "Age_At_Diagnosis                                    0.024481   \n",
       "Regional_Lymph_Nodes_examined                       1.000000   \n",
       "Regional_Lymph_Nodes_positive                       0.214147   \n",
       "\n",
       "                               Regional_Lymph_Nodes_positive  \\\n",
       "Year_of_Diagnosis                                   0.116608   \n",
       "Tumor_Sequence_Number                               0.096182   \n",
       "Age_At_Diagnosis                                    0.083128   \n",
       "Regional_Lymph_Nodes_examined                       0.214147   \n",
       "Regional_Lymph_Nodes_positive                       1.000000   \n",
       "\n",
       "                               Year_of_Last_Contact_or_Death__S  \\\n",
       "Year_of_Diagnosis                                      0.662213   \n",
       "Tumor_Sequence_Number                                  0.114551   \n",
       "Age_At_Diagnosis                                       0.089446   \n",
       "Regional_Lymph_Nodes_examined                          0.106061   \n",
       "Regional_Lymph_Nodes_positive                          0.014539   \n",
       "\n",
       "                               Year_of_Last_Contact_or_Death__F  \\\n",
       "Year_of_Diagnosis                                      0.656449   \n",
       "Tumor_Sequence_Number                                  0.127843   \n",
       "Age_At_Diagnosis                                       0.082757   \n",
       "Regional_Lymph_Nodes_examined                          0.105154   \n",
       "Regional_Lymph_Nodes_positive                          0.015377   \n",
       "\n",
       "                               Overall_Survival_1__Days_  \\\n",
       "Year_of_Diagnosis                               0.284000   \n",
       "Tumor_Sequence_Number                           0.044502   \n",
       "Age_At_Diagnosis                                0.204286   \n",
       "Regional_Lymph_Nodes_examined                   0.204488   \n",
       "Regional_Lymph_Nodes_positive                   0.165666   \n",
       "\n",
       "                               Overall_Survival_1__Months_  \\\n",
       "Year_of_Diagnosis                                 0.283572   \n",
       "Tumor_Sequence_Number                             0.044990   \n",
       "Age_At_Diagnosis                                  0.204993   \n",
       "Regional_Lymph_Nodes_examined                     0.203869   \n",
       "Regional_Lymph_Nodes_positive                     0.164628   \n",
       "\n",
       "                               Overall_Survival_2__Days_  ...      8300  \\\n",
       "Year_of_Diagnosis                               0.285061  ...  0.096738   \n",
       "Tumor_Sequence_Number                           0.046955  ...  0.020590   \n",
       "Age_At_Diagnosis                                0.202093  ...  0.105379   \n",
       "Regional_Lymph_Nodes_examined                   0.202699  ...  0.162477   \n",
       "Regional_Lymph_Nodes_positive                   0.164375  ...  0.126528   \n",
       "\n",
       "                                   8301      8304      8305      8307  \\\n",
       "Year_of_Diagnosis              0.024911  0.015518  0.042033  0.051857   \n",
       "Tumor_Sequence_Number          0.039268  0.005099  0.044364  0.073432   \n",
       "Age_At_Diagnosis               0.011552  0.000181  0.017555  0.027586   \n",
       "Regional_Lymph_Nodes_examined  0.144585  0.068971  0.095920  0.075105   \n",
       "Regional_Lymph_Nodes_positive  0.052517  0.043171  0.004640  0.122362   \n",
       "\n",
       "                                   8309      8310      8311      8314  \\\n",
       "Year_of_Diagnosis              0.170086  0.005412  0.049978  0.072603   \n",
       "Tumor_Sequence_Number          0.068132  0.060961  0.075107  0.110043   \n",
       "Age_At_Diagnosis               0.030560  0.007127  0.134809  0.016448   \n",
       "Regional_Lymph_Nodes_examined  0.026212  0.079932  0.013017  0.076088   \n",
       "Regional_Lymph_Nodes_positive  0.027579  0.049116  0.109744  0.073299   \n",
       "\n",
       "                                   8316  \n",
       "Year_of_Diagnosis              0.090380  \n",
       "Tumor_Sequence_Number          0.062217  \n",
       "Age_At_Diagnosis               0.107989  \n",
       "Regional_Lymph_Nodes_examined  0.032443  \n",
       "Regional_Lymph_Nodes_positive  0.080292  \n",
       "\n",
       "[5 rows x 4477 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Threshold for removing correlated variables\n",
    "threshold = 0.75\n",
    "\n",
    "# Absolute value correlation matrix\n",
    "corr_matrix = x.corr().abs()\n",
    "print(corr_matrix.shape)\n",
    "corr_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2e16d745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4477, 4477)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_of_Diagnosis</th>\n",
       "      <th>Tumor_Sequence_Number</th>\n",
       "      <th>Age_At_Diagnosis</th>\n",
       "      <th>Regional_Lymph_Nodes_examined</th>\n",
       "      <th>Regional_Lymph_Nodes_positive</th>\n",
       "      <th>Year_of_Last_Contact_or_Death__S</th>\n",
       "      <th>Year_of_Last_Contact_or_Death__F</th>\n",
       "      <th>Overall_Survival_1__Days_</th>\n",
       "      <th>Overall_Survival_1__Months_</th>\n",
       "      <th>Overall_Survival_2__Days_</th>\n",
       "      <th>...</th>\n",
       "      <th>8300</th>\n",
       "      <th>8301</th>\n",
       "      <th>8304</th>\n",
       "      <th>8305</th>\n",
       "      <th>8307</th>\n",
       "      <th>8309</th>\n",
       "      <th>8310</th>\n",
       "      <th>8311</th>\n",
       "      <th>8314</th>\n",
       "      <th>8316</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Year_of_Diagnosis</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.100609</td>\n",
       "      <td>0.074038</td>\n",
       "      <td>0.052394</td>\n",
       "      <td>0.116608</td>\n",
       "      <td>0.662213</td>\n",
       "      <td>0.656449</td>\n",
       "      <td>0.284000</td>\n",
       "      <td>0.283572</td>\n",
       "      <td>0.285061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096738</td>\n",
       "      <td>0.024911</td>\n",
       "      <td>0.015518</td>\n",
       "      <td>0.042033</td>\n",
       "      <td>0.051857</td>\n",
       "      <td>0.170086</td>\n",
       "      <td>0.005412</td>\n",
       "      <td>0.049978</td>\n",
       "      <td>0.072603</td>\n",
       "      <td>0.090380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tumor_Sequence_Number</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.265371</td>\n",
       "      <td>0.197846</td>\n",
       "      <td>0.096182</td>\n",
       "      <td>0.114551</td>\n",
       "      <td>0.127843</td>\n",
       "      <td>0.044502</td>\n",
       "      <td>0.044990</td>\n",
       "      <td>0.046955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020590</td>\n",
       "      <td>0.039268</td>\n",
       "      <td>0.005099</td>\n",
       "      <td>0.044364</td>\n",
       "      <td>0.073432</td>\n",
       "      <td>0.068132</td>\n",
       "      <td>0.060961</td>\n",
       "      <td>0.075107</td>\n",
       "      <td>0.110043</td>\n",
       "      <td>0.062217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age_At_Diagnosis</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.024481</td>\n",
       "      <td>0.083128</td>\n",
       "      <td>0.089446</td>\n",
       "      <td>0.082757</td>\n",
       "      <td>0.204286</td>\n",
       "      <td>0.204993</td>\n",
       "      <td>0.202093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105379</td>\n",
       "      <td>0.011552</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.017555</td>\n",
       "      <td>0.027586</td>\n",
       "      <td>0.030560</td>\n",
       "      <td>0.007127</td>\n",
       "      <td>0.134809</td>\n",
       "      <td>0.016448</td>\n",
       "      <td>0.107989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Regional_Lymph_Nodes_examined</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.214147</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.105154</td>\n",
       "      <td>0.204488</td>\n",
       "      <td>0.203869</td>\n",
       "      <td>0.202699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162477</td>\n",
       "      <td>0.144585</td>\n",
       "      <td>0.068971</td>\n",
       "      <td>0.095920</td>\n",
       "      <td>0.075105</td>\n",
       "      <td>0.026212</td>\n",
       "      <td>0.079932</td>\n",
       "      <td>0.013017</td>\n",
       "      <td>0.076088</td>\n",
       "      <td>0.032443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Regional_Lymph_Nodes_positive</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014539</td>\n",
       "      <td>0.015377</td>\n",
       "      <td>0.165666</td>\n",
       "      <td>0.164628</td>\n",
       "      <td>0.164375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126528</td>\n",
       "      <td>0.052517</td>\n",
       "      <td>0.043171</td>\n",
       "      <td>0.004640</td>\n",
       "      <td>0.122362</td>\n",
       "      <td>0.027579</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.109744</td>\n",
       "      <td>0.073299</td>\n",
       "      <td>0.080292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4477 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Year_of_Diagnosis  Tumor_Sequence_Number  \\\n",
       "Year_of_Diagnosis                            NaN               0.100609   \n",
       "Tumor_Sequence_Number                        NaN                    NaN   \n",
       "Age_At_Diagnosis                             NaN                    NaN   \n",
       "Regional_Lymph_Nodes_examined                NaN                    NaN   \n",
       "Regional_Lymph_Nodes_positive                NaN                    NaN   \n",
       "\n",
       "                               Age_At_Diagnosis  \\\n",
       "Year_of_Diagnosis                      0.074038   \n",
       "Tumor_Sequence_Number                  0.265371   \n",
       "Age_At_Diagnosis                            NaN   \n",
       "Regional_Lymph_Nodes_examined               NaN   \n",
       "Regional_Lymph_Nodes_positive               NaN   \n",
       "\n",
       "                               Regional_Lymph_Nodes_examined  \\\n",
       "Year_of_Diagnosis                                   0.052394   \n",
       "Tumor_Sequence_Number                               0.197846   \n",
       "Age_At_Diagnosis                                    0.024481   \n",
       "Regional_Lymph_Nodes_examined                            NaN   \n",
       "Regional_Lymph_Nodes_positive                            NaN   \n",
       "\n",
       "                               Regional_Lymph_Nodes_positive  \\\n",
       "Year_of_Diagnosis                                   0.116608   \n",
       "Tumor_Sequence_Number                               0.096182   \n",
       "Age_At_Diagnosis                                    0.083128   \n",
       "Regional_Lymph_Nodes_examined                       0.214147   \n",
       "Regional_Lymph_Nodes_positive                            NaN   \n",
       "\n",
       "                               Year_of_Last_Contact_or_Death__S  \\\n",
       "Year_of_Diagnosis                                      0.662213   \n",
       "Tumor_Sequence_Number                                  0.114551   \n",
       "Age_At_Diagnosis                                       0.089446   \n",
       "Regional_Lymph_Nodes_examined                          0.106061   \n",
       "Regional_Lymph_Nodes_positive                          0.014539   \n",
       "\n",
       "                               Year_of_Last_Contact_or_Death__F  \\\n",
       "Year_of_Diagnosis                                      0.656449   \n",
       "Tumor_Sequence_Number                                  0.127843   \n",
       "Age_At_Diagnosis                                       0.082757   \n",
       "Regional_Lymph_Nodes_examined                          0.105154   \n",
       "Regional_Lymph_Nodes_positive                          0.015377   \n",
       "\n",
       "                               Overall_Survival_1__Days_  \\\n",
       "Year_of_Diagnosis                               0.284000   \n",
       "Tumor_Sequence_Number                           0.044502   \n",
       "Age_At_Diagnosis                                0.204286   \n",
       "Regional_Lymph_Nodes_examined                   0.204488   \n",
       "Regional_Lymph_Nodes_positive                   0.165666   \n",
       "\n",
       "                               Overall_Survival_1__Months_  \\\n",
       "Year_of_Diagnosis                                 0.283572   \n",
       "Tumor_Sequence_Number                             0.044990   \n",
       "Age_At_Diagnosis                                  0.204993   \n",
       "Regional_Lymph_Nodes_examined                     0.203869   \n",
       "Regional_Lymph_Nodes_positive                     0.164628   \n",
       "\n",
       "                               Overall_Survival_2__Days_  ...      8300  \\\n",
       "Year_of_Diagnosis                               0.285061  ...  0.096738   \n",
       "Tumor_Sequence_Number                           0.046955  ...  0.020590   \n",
       "Age_At_Diagnosis                                0.202093  ...  0.105379   \n",
       "Regional_Lymph_Nodes_examined                   0.202699  ...  0.162477   \n",
       "Regional_Lymph_Nodes_positive                   0.164375  ...  0.126528   \n",
       "\n",
       "                                   8301      8304      8305      8307  \\\n",
       "Year_of_Diagnosis              0.024911  0.015518  0.042033  0.051857   \n",
       "Tumor_Sequence_Number          0.039268  0.005099  0.044364  0.073432   \n",
       "Age_At_Diagnosis               0.011552  0.000181  0.017555  0.027586   \n",
       "Regional_Lymph_Nodes_examined  0.144585  0.068971  0.095920  0.075105   \n",
       "Regional_Lymph_Nodes_positive  0.052517  0.043171  0.004640  0.122362   \n",
       "\n",
       "                                   8309      8310      8311      8314  \\\n",
       "Year_of_Diagnosis              0.170086  0.005412  0.049978  0.072603   \n",
       "Tumor_Sequence_Number          0.068132  0.060961  0.075107  0.110043   \n",
       "Age_At_Diagnosis               0.030560  0.007127  0.134809  0.016448   \n",
       "Regional_Lymph_Nodes_examined  0.026212  0.079932  0.013017  0.076088   \n",
       "Regional_Lymph_Nodes_positive  0.027579  0.049116  0.109744  0.073299   \n",
       "\n",
       "                                   8316  \n",
       "Year_of_Diagnosis              0.090380  \n",
       "Tumor_Sequence_Number          0.062217  \n",
       "Age_At_Diagnosis               0.107989  \n",
       "Regional_Lymph_Nodes_examined  0.032443  \n",
       "Regional_Lymph_Nodes_positive  0.080292  \n",
       "\n",
       "[5 rows x 4477 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upper triangle of correlations\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "print(upper.shape)\n",
    "upper.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fa2cc716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2283 columns to remove.\n"
     ]
    }
   ],
   "source": [
    "# Select columns with correlations above threshold\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "print('There are %d columns to remove.' % (len(to_drop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9586c9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape:  (103, 2194)\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_of_Diagnosis</th>\n",
       "      <th>Tumor_Sequence_Number</th>\n",
       "      <th>Age_At_Diagnosis</th>\n",
       "      <th>Regional_Lymph_Nodes_examined</th>\n",
       "      <th>Regional_Lymph_Nodes_positive</th>\n",
       "      <th>Year_of_Last_Contact_or_Death__S</th>\n",
       "      <th>Overall_Survival_1__Days_</th>\n",
       "      <th>Days_diff_b_w_Diagnosis_and_Spec</th>\n",
       "      <th>Days_diff_b_w_date_of_last_conta</th>\n",
       "      <th>Recurrence__Final_</th>\n",
       "      <th>...</th>\n",
       "      <th>8205</th>\n",
       "      <th>8216</th>\n",
       "      <th>8218</th>\n",
       "      <th>8223</th>\n",
       "      <th>8228</th>\n",
       "      <th>8251</th>\n",
       "      <th>8262</th>\n",
       "      <th>8279</th>\n",
       "      <th>8294</th>\n",
       "      <th>8295</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2740</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.241668</td>\n",
       "      <td>14.947996</td>\n",
       "      <td>15.521010</td>\n",
       "      <td>17.433051</td>\n",
       "      <td>17.812551</td>\n",
       "      <td>17.708263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1998</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1999</td>\n",
       "      <td>459</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.295241</td>\n",
       "      <td>15.556385</td>\n",
       "      <td>17.563705</td>\n",
       "      <td>18.780560</td>\n",
       "      <td>18.284464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.571433</td>\n",
       "      <td>15.924591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1998</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>6230</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.159331</td>\n",
       "      <td>15.708085</td>\n",
       "      <td>17.239521</td>\n",
       "      <td>19.302844</td>\n",
       "      <td>17.885742</td>\n",
       "      <td>13.515277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "      <td>2825</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.249026</td>\n",
       "      <td>16.393885</td>\n",
       "      <td>16.387141</td>\n",
       "      <td>19.165422</td>\n",
       "      <td>18.282855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.260503</td>\n",
       "      <td>16.554338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2537</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.944052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.924618</td>\n",
       "      <td>18.138895</td>\n",
       "      <td>18.298642</td>\n",
       "      <td>13.787291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.521409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year_of_Diagnosis  Tumor_Sequence_Number  Age_At_Diagnosis  \\\n",
       "0               2008                      0                64   \n",
       "1               1998                      0                69   \n",
       "2               1998                      0                66   \n",
       "3               2004                      0                72   \n",
       "4               2009                      0                52   \n",
       "\n",
       "   Regional_Lymph_Nodes_examined  Regional_Lymph_Nodes_positive  \\\n",
       "0                             11                              0   \n",
       "1                             12                              0   \n",
       "2                             10                              0   \n",
       "3                             25                              0   \n",
       "4                             18                              0   \n",
       "\n",
       "   Year_of_Last_Contact_or_Death__S  Overall_Survival_1__Days_  \\\n",
       "0                              2015                       2740   \n",
       "1                              1999                        459   \n",
       "2                              2015                       6230   \n",
       "3                              2012                       2825   \n",
       "4                              2015                       2537   \n",
       "\n",
       "   Days_diff_b_w_Diagnosis_and_Spec  Days_diff_b_w_date_of_last_conta  \\\n",
       "0                               141                                 0   \n",
       "1                                31                                 0   \n",
       "2                                60                                 0   \n",
       "3                                 0                                 0   \n",
       "4                                21                                 0   \n",
       "\n",
       "   Recurrence__Final_  ...  8205       8216       8218       8223       8228  \\\n",
       "0                   0  ...   0.0  16.241668  14.947996  15.521010  17.433051   \n",
       "1                   1  ...   0.0   0.000000  14.295241  15.556385  17.563705   \n",
       "2                   0  ...   0.0   0.000000  15.159331  15.708085  17.239521   \n",
       "3                   0  ...   0.0   0.000000  14.249026  16.393885  16.387141   \n",
       "4                   0  ...   0.0   0.000000  14.944052   0.000000  16.924618   \n",
       "\n",
       "        8251       8262       8279       8294       8295  \n",
       "0  17.812551  17.708263   0.000000   0.000000   0.000000  \n",
       "1  18.780560  18.284464   0.000000  16.571433  15.924591  \n",
       "2  19.302844  17.885742  13.515277   0.000000   0.000000  \n",
       "3  19.165422  18.282855   0.000000  17.260503  16.554338  \n",
       "4  18.138895  18.298642  13.787291   0.000000  16.521409  \n",
       "\n",
       "[5 rows x 2194 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_uncorr = x.drop(columns = to_drop)\n",
    "print('Training shape: ', df_uncorr.shape)\n",
    "print(type(df_uncorr))\n",
    "# print the head of the new dataframe\n",
    "df_uncorr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e1e36449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features:  103\n",
      "Numerical features:  2194\n"
     ]
    }
   ],
   "source": [
    "# print categorical features\n",
    "print('Categorical features: ', len(df_categorical_features))\n",
    "# print numerical features\n",
    "print('Numerical features: ', len(df_uncorr.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fd8840f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape:  (103, 2224)\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_of_Diagnosis</th>\n",
       "      <th>Tumor_Sequence_Number</th>\n",
       "      <th>Age_At_Diagnosis</th>\n",
       "      <th>Regional_Lymph_Nodes_examined</th>\n",
       "      <th>Regional_Lymph_Nodes_positive</th>\n",
       "      <th>Year_of_Last_Contact_or_Death__S</th>\n",
       "      <th>Overall_Survival_1__Days_</th>\n",
       "      <th>Days_diff_b_w_Diagnosis_and_Spec</th>\n",
       "      <th>Days_diff_b_w_date_of_last_conta</th>\n",
       "      <th>Recurrence__Final_</th>\n",
       "      <th>...</th>\n",
       "      <th>subtype.1</th>\n",
       "      <th>Tobacco use (FCDS) [Data element retired in 2011]</th>\n",
       "      <th>Recently Reported Cigarette Smoking Status</th>\n",
       "      <th>Recently Reported Tobacco Use</th>\n",
       "      <th>Trypsin.lot.number</th>\n",
       "      <th>Digestion.Start.Time</th>\n",
       "      <th>Digestion.End.Time</th>\n",
       "      <th>TMT.Sample.Name</th>\n",
       "      <th>Proteomic Subtype</th>\n",
       "      <th>Wilkerson Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2740</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Inflamed</td>\n",
       "      <td>MODERATE (1-2 PPD)</td>\n",
       "      <td>EVER</td>\n",
       "      <td>EVER</td>\n",
       "      <td>R5E15825</td>\n",
       "      <td>2:34 PM</td>\n",
       "      <td>11:35 AM</td>\n",
       "      <td>TMT26_TMT-130</td>\n",
       "      <td>Inflamed A</td>\n",
       "      <td>classical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1998</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1999</td>\n",
       "      <td>459</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Inflamed</td>\n",
       "      <td>HIST</td>\n",
       "      <td>EVER</td>\n",
       "      <td>EVER</td>\n",
       "      <td>R5E15825</td>\n",
       "      <td>1:00 PM</td>\n",
       "      <td>11:25 AM</td>\n",
       "      <td>TMT23_TMT-131</td>\n",
       "      <td>Inflamed A</td>\n",
       "      <td>secretory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1998</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>6230</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Inflamed</td>\n",
       "      <td>HIST</td>\n",
       "      <td>EVER</td>\n",
       "      <td>EVER</td>\n",
       "      <td>R5E15825</td>\n",
       "      <td>1:00 PM</td>\n",
       "      <td>11:25 AM</td>\n",
       "      <td>TMT24_TMT-128</td>\n",
       "      <td>Inflamed A</td>\n",
       "      <td>secretory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "      <td>2825</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Inflamed</td>\n",
       "      <td>NONE</td>\n",
       "      <td>EVER</td>\n",
       "      <td>EVER</td>\n",
       "      <td>R5E15825</td>\n",
       "      <td>1:00 PM</td>\n",
       "      <td>11:25 AM</td>\n",
       "      <td>TMT23_TMT-127</td>\n",
       "      <td>Inflamed A</td>\n",
       "      <td>primitive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2537</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Inflamed</td>\n",
       "      <td>HIST</td>\n",
       "      <td>EVER</td>\n",
       "      <td>EVER</td>\n",
       "      <td>R5E15825</td>\n",
       "      <td>1:54 PM</td>\n",
       "      <td>11:12 AM</td>\n",
       "      <td>TMT10_TMT-127</td>\n",
       "      <td>Inflamed A</td>\n",
       "      <td>classical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2224 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year_of_Diagnosis  Tumor_Sequence_Number  Age_At_Diagnosis  \\\n",
       "0               2008                      0                64   \n",
       "1               1998                      0                69   \n",
       "2               1998                      0                66   \n",
       "3               2004                      0                72   \n",
       "4               2009                      0                52   \n",
       "\n",
       "   Regional_Lymph_Nodes_examined  Regional_Lymph_Nodes_positive  \\\n",
       "0                             11                              0   \n",
       "1                             12                              0   \n",
       "2                             10                              0   \n",
       "3                             25                              0   \n",
       "4                             18                              0   \n",
       "\n",
       "   Year_of_Last_Contact_or_Death__S  Overall_Survival_1__Days_  \\\n",
       "0                              2015                       2740   \n",
       "1                              1999                        459   \n",
       "2                              2015                       6230   \n",
       "3                              2012                       2825   \n",
       "4                              2015                       2537   \n",
       "\n",
       "   Days_diff_b_w_Diagnosis_and_Spec  Days_diff_b_w_date_of_last_conta  \\\n",
       "0                               141                                 0   \n",
       "1                                31                                 0   \n",
       "2                                60                                 0   \n",
       "3                                 0                                 0   \n",
       "4                                21                                 0   \n",
       "\n",
       "   Recurrence__Final_  ...  subtype.1  \\\n",
       "0                   0  ...   Inflamed   \n",
       "1                   1  ...   Inflamed   \n",
       "2                   0  ...   Inflamed   \n",
       "3                   0  ...   Inflamed   \n",
       "4                   0  ...   Inflamed   \n",
       "\n",
       "   Tobacco use (FCDS) [Data element retired in 2011]  \\\n",
       "0                                 MODERATE (1-2 PPD)   \n",
       "1                                               HIST   \n",
       "2                                               HIST   \n",
       "3                                               NONE   \n",
       "4                                               HIST   \n",
       "\n",
       "   Recently Reported Cigarette Smoking Status  Recently Reported Tobacco Use  \\\n",
       "0                                        EVER                           EVER   \n",
       "1                                        EVER                           EVER   \n",
       "2                                        EVER                           EVER   \n",
       "3                                        EVER                           EVER   \n",
       "4                                        EVER                           EVER   \n",
       "\n",
       "   Trypsin.lot.number  Digestion.Start.Time  Digestion.End.Time  \\\n",
       "0            R5E15825               2:34 PM            11:35 AM   \n",
       "1            R5E15825               1:00 PM            11:25 AM   \n",
       "2            R5E15825               1:00 PM            11:25 AM   \n",
       "3            R5E15825               1:00 PM            11:25 AM   \n",
       "4            R5E15825               1:54 PM            11:12 AM   \n",
       "\n",
       "   TMT.Sample.Name  Proteomic Subtype  Wilkerson Classification  \n",
       "0    TMT26_TMT-130         Inflamed A                 classical  \n",
       "1    TMT23_TMT-131         Inflamed A                 secretory  \n",
       "2    TMT24_TMT-128         Inflamed A                 secretory  \n",
       "3    TMT23_TMT-127         Inflamed A                 primitive  \n",
       "4    TMT10_TMT-127         Inflamed A                 classical  \n",
       "\n",
       "[5 rows x 2224 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the numerical features and categorical features\n",
    "df_uncorr = pd.concat([df_uncorr, df_categorical_features], axis=1)\n",
    "print('Training shape: ', df_uncorr.shape)\n",
    "print(type(df_uncorr))\n",
    "# print the head of the new dataframe\n",
    "df_uncorr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8d2e6685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape:  (103, 2516)\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_of_Diagnosis</th>\n",
       "      <th>Tumor_Sequence_Number</th>\n",
       "      <th>Age_At_Diagnosis</th>\n",
       "      <th>Regional_Lymph_Nodes_examined</th>\n",
       "      <th>Regional_Lymph_Nodes_positive</th>\n",
       "      <th>Year_of_Last_Contact_or_Death__S</th>\n",
       "      <th>Overall_Survival_1__Days_</th>\n",
       "      <th>Days_diff_b_w_Diagnosis_and_Spec</th>\n",
       "      <th>Days_diff_b_w_date_of_last_conta</th>\n",
       "      <th>Recurrence__Final_</th>\n",
       "      <th>...</th>\n",
       "      <th>TMT.Sample.Name_TMT29_TMT-131</th>\n",
       "      <th>Proteomic Subtype_Inflamed A</th>\n",
       "      <th>Proteomic Subtype_Inflamed B</th>\n",
       "      <th>Proteomic Subtype_Mixed</th>\n",
       "      <th>Proteomic Subtype_Redox A</th>\n",
       "      <th>Proteomic Subtype_Redox B</th>\n",
       "      <th>Wilkerson Classification_basal</th>\n",
       "      <th>Wilkerson Classification_classical</th>\n",
       "      <th>Wilkerson Classification_primitive</th>\n",
       "      <th>Wilkerson Classification_secretory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2740</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1998</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1999</td>\n",
       "      <td>459</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1998</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>6230</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "      <td>2825</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2537</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2516 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year_of_Diagnosis  Tumor_Sequence_Number  Age_At_Diagnosis  \\\n",
       "0               2008                      0                64   \n",
       "1               1998                      0                69   \n",
       "2               1998                      0                66   \n",
       "3               2004                      0                72   \n",
       "4               2009                      0                52   \n",
       "\n",
       "   Regional_Lymph_Nodes_examined  Regional_Lymph_Nodes_positive  \\\n",
       "0                             11                              0   \n",
       "1                             12                              0   \n",
       "2                             10                              0   \n",
       "3                             25                              0   \n",
       "4                             18                              0   \n",
       "\n",
       "   Year_of_Last_Contact_or_Death__S  Overall_Survival_1__Days_  \\\n",
       "0                              2015                       2740   \n",
       "1                              1999                        459   \n",
       "2                              2015                       6230   \n",
       "3                              2012                       2825   \n",
       "4                              2015                       2537   \n",
       "\n",
       "   Days_diff_b_w_Diagnosis_and_Spec  Days_diff_b_w_date_of_last_conta  \\\n",
       "0                               141                                 0   \n",
       "1                                31                                 0   \n",
       "2                                60                                 0   \n",
       "3                                 0                                 0   \n",
       "4                                21                                 0   \n",
       "\n",
       "   Recurrence__Final_  ...  TMT.Sample.Name_TMT29_TMT-131  \\\n",
       "0                   0  ...                              0   \n",
       "1                   1  ...                              0   \n",
       "2                   0  ...                              0   \n",
       "3                   0  ...                              0   \n",
       "4                   0  ...                              0   \n",
       "\n",
       "   Proteomic Subtype_Inflamed A  Proteomic Subtype_Inflamed B  \\\n",
       "0                             1                             0   \n",
       "1                             1                             0   \n",
       "2                             1                             0   \n",
       "3                             1                             0   \n",
       "4                             1                             0   \n",
       "\n",
       "   Proteomic Subtype_Mixed  Proteomic Subtype_Redox A  \\\n",
       "0                        0                          0   \n",
       "1                        0                          0   \n",
       "2                        0                          0   \n",
       "3                        0                          0   \n",
       "4                        0                          0   \n",
       "\n",
       "   Proteomic Subtype_Redox B  Wilkerson Classification_basal  \\\n",
       "0                          0                               0   \n",
       "1                          0                               0   \n",
       "2                          0                               0   \n",
       "3                          0                               0   \n",
       "4                          0                               0   \n",
       "\n",
       "   Wilkerson Classification_classical  Wilkerson Classification_primitive  \\\n",
       "0                                   1                                   0   \n",
       "1                                   0                                   0   \n",
       "2                                   0                                   0   \n",
       "3                                   0                                   1   \n",
       "4                                   1                                   0   \n",
       "\n",
       "   Wilkerson Classification_secretory  \n",
       "0                                   0  \n",
       "1                                   1  \n",
       "2                                   1  \n",
       "3                                   0  \n",
       "4                                   0  \n",
       "\n",
       "[5 rows x 2516 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encoding\n",
    "df_uncorr = pd.get_dummies(df_uncorr)\n",
    "print('Training shape: ', df_uncorr.shape)\n",
    "print(type(df_uncorr))\n",
    "# print the head of the new dataframe\n",
    "df_uncorr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2a48efe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Overall_Survival_1__Days_'], dtype='object')\n",
      "Index(['sample_name_SCC001', 'sample_name_SCC002', 'sample_name_SCC003',\n",
      "       'sample_name_SCC004', 'sample_name_SCC005', 'sample_name_SCC006',\n",
      "       'sample_name_SCC007', 'sample_name_SCC008', 'sample_name_SCC009',\n",
      "       'sample_name_SCC010',\n",
      "       ...\n",
      "       'sample_name_SCC098', 'sample_name_SCC099', 'sample_name_SCC100',\n",
      "       'sample_name_SCC101', 'sample_name_SCC103', 'sample_name_SCC104',\n",
      "       'sample_name_SCC105', 'sample_name_SCC106', 'sample_name_SCC107',\n",
      "       'sample_name_SCC108'],\n",
      "      dtype='object', length=103)\n"
     ]
    }
   ],
   "source": [
    "# Print columns having names with 'Overall_Survival'\n",
    "print(df_uncorr.columns[df_uncorr.columns.str.contains('Survival')])\n",
    "print(df_uncorr.columns[df_uncorr.columns.str.contains('sample_name')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9a3bde91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 103 columns that contain sample_name\n"
     ]
    }
   ],
   "source": [
    "# Remove the one-hot columns having sample names\n",
    "cols_with_id = [x for x in df_uncorr.columns if 'sample_name' in x]\n",
    "print('There are %d columns that contain sample_name' % len(cols_with_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "90f72a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape:  (103, 2413)\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Drop the columns with sample_name\n",
    "df_data = df_uncorr.drop(columns = cols_with_id)\n",
    "print('Training shape: ', df_data.shape)\n",
    "print(type(df_data))\n",
    "# print the head of the new dataframe\n",
    "df_data.head()\n",
    "# save the dataframe to a .xlxs file\n",
    "df_data.to_excel('data/Data-Processed_uncorr.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "69645072",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    44\n",
      "0    31\n",
      "1    28\n",
      "Name: Overall_Survival_1__Days_, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_of_Diagnosis</th>\n",
       "      <th>Tumor_Sequence_Number</th>\n",
       "      <th>Age_At_Diagnosis</th>\n",
       "      <th>Regional_Lymph_Nodes_examined</th>\n",
       "      <th>Regional_Lymph_Nodes_positive</th>\n",
       "      <th>Year_of_Last_Contact_or_Death__S</th>\n",
       "      <th>Overall_Survival_1__Days_</th>\n",
       "      <th>Days_diff_b_w_Diagnosis_and_Spec</th>\n",
       "      <th>Days_diff_b_w_date_of_last_conta</th>\n",
       "      <th>Recurrence__Final_</th>\n",
       "      <th>...</th>\n",
       "      <th>TMT.Sample.Name_TMT29_TMT-131</th>\n",
       "      <th>Proteomic Subtype_Inflamed A</th>\n",
       "      <th>Proteomic Subtype_Inflamed B</th>\n",
       "      <th>Proteomic Subtype_Mixed</th>\n",
       "      <th>Proteomic Subtype_Redox A</th>\n",
       "      <th>Proteomic Subtype_Redox B</th>\n",
       "      <th>Wilkerson Classification_basal</th>\n",
       "      <th>Wilkerson Classification_classical</th>\n",
       "      <th>Wilkerson Classification_primitive</th>\n",
       "      <th>Wilkerson Classification_secretory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1998</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1999</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1998</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2413 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year_of_Diagnosis  Tumor_Sequence_Number  Age_At_Diagnosis  \\\n",
       "0               2008                      0                64   \n",
       "1               1998                      0                69   \n",
       "2               1998                      0                66   \n",
       "3               2004                      0                72   \n",
       "4               2009                      0                52   \n",
       "\n",
       "   Regional_Lymph_Nodes_examined  Regional_Lymph_Nodes_positive  \\\n",
       "0                             11                              0   \n",
       "1                             12                              0   \n",
       "2                             10                              0   \n",
       "3                             25                              0   \n",
       "4                             18                              0   \n",
       "\n",
       "   Year_of_Last_Contact_or_Death__S  Overall_Survival_1__Days_  \\\n",
       "0                              2015                          2   \n",
       "1                              1999                          0   \n",
       "2                              2015                          2   \n",
       "3                              2012                          2   \n",
       "4                              2015                          2   \n",
       "\n",
       "   Days_diff_b_w_Diagnosis_and_Spec  Days_diff_b_w_date_of_last_conta  \\\n",
       "0                               141                                 0   \n",
       "1                                31                                 0   \n",
       "2                                60                                 0   \n",
       "3                                 0                                 0   \n",
       "4                                21                                 0   \n",
       "\n",
       "   Recurrence__Final_  ...  TMT.Sample.Name_TMT29_TMT-131  \\\n",
       "0                   0  ...                              0   \n",
       "1                   1  ...                              0   \n",
       "2                   0  ...                              0   \n",
       "3                   0  ...                              0   \n",
       "4                   0  ...                              0   \n",
       "\n",
       "   Proteomic Subtype_Inflamed A  Proteomic Subtype_Inflamed B  \\\n",
       "0                             1                             0   \n",
       "1                             1                             0   \n",
       "2                             1                             0   \n",
       "3                             1                             0   \n",
       "4                             1                             0   \n",
       "\n",
       "   Proteomic Subtype_Mixed  Proteomic Subtype_Redox A  \\\n",
       "0                        0                          0   \n",
       "1                        0                          0   \n",
       "2                        0                          0   \n",
       "3                        0                          0   \n",
       "4                        0                          0   \n",
       "\n",
       "   Proteomic Subtype_Redox B  Wilkerson Classification_basal  \\\n",
       "0                          0                               0   \n",
       "1                          0                               0   \n",
       "2                          0                               0   \n",
       "3                          0                               0   \n",
       "4                          0                               0   \n",
       "\n",
       "   Wilkerson Classification_classical  Wilkerson Classification_primitive  \\\n",
       "0                                   1                                   0   \n",
       "1                                   0                                   0   \n",
       "2                                   0                                   0   \n",
       "3                                   0                                   1   \n",
       "4                                   1                                   0   \n",
       "\n",
       "   Wilkerson Classification_secretory  \n",
       "0                                   0  \n",
       "1                                   1  \n",
       "2                                   1  \n",
       "3                                   0  \n",
       "4                                   0  \n",
       "\n",
       "[5 rows x 2413 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Class labels for training\n",
    "# convert the df_data['Overall_Survival_1__Days_'] column values to three classes based on thresholds\n",
    "labels = np.array(df_data['Overall_Survival_1__Days_'])\n",
    "labels[labels <= 730] = 0   # 2 years\n",
    "labels[(labels > 730) & (labels <= 2190)] = 1 # between 2 and 6 years\n",
    "labels[labels > 2190] = 2   # greater than 6 years\n",
    "df_data['Overall_Survival_1__Days_'] = labels\n",
    "\n",
    "print(df_data['Overall_Survival_1__Days_'].value_counts())\n",
    "# print(df_data['Overall_Survival_1__Days_'])\n",
    "# save the dataframe to a .xlxs file\n",
    "df_data.to_excel('data/Data-Processed_uncorr_class-labled.xlsx', index=False)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a9d28d",
   "metadata": {},
   "source": [
    "### Training, Validation Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f1dfb2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is split into 5 stratified training and validation folds with each set saved to new csv files for later use.\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "target = df_data['Overall_Survival_1__Days_']\n",
    "fold_no=1\n",
    "for train_index, test_index in skf.split(df_data, target):\n",
    "    train = df_data.iloc[train_index]\n",
    "    test = df_data.iloc[test_index]\n",
    "    train.to_csv('data/train_fold_{}.csv'.format(fold_no), index=False)\n",
    "    test.to_csv('data/test_fold_{}.csv'.format(fold_no), index=False)\n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "59a2c730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1\n",
      "Fold # 1 features and lables saved to csv files in folder: data/fold1\n",
      "Fold:  2\n",
      "Fold # 2 features and lables saved to csv files in folder: data/fold2\n",
      "Fold:  3\n",
      "Fold # 3 features and lables saved to csv files in folder: data/fold3\n",
      "Fold:  4\n",
      "Fold # 4 features and lables saved to csv files in folder: data/fold4\n",
      "Fold:  5\n",
      "Fold # 5 features and lables saved to csv files in folder: data/fold5\n"
     ]
    }
   ],
   "source": [
    "# load all kfold files and create training and testing labels\n",
    "for i in range(1, 6):\n",
    "    train = pd.read_csv('data/folds/training_folds/fold{}/train_fold_{}.csv'.format(i,i))\n",
    "    test = pd.read_csv('data/folds/validation_folds/fold{}/test_fold_{}.csv'.format(i,i))\n",
    "    train_labels = np.array(train['Overall_Survival_1__Days_'])\n",
    "    test_labels = np.array(test['Overall_Survival_1__Days_'])\n",
    "    print('Fold: ', i)\n",
    "    # Remove the labels from the features\n",
    "    # axis 1 refers to the columns\n",
    "    train = train.drop('Overall_Survival_1__Days_', axis = 1)\n",
    "    test = test.drop('Overall_Survival_1__Days_', axis = 1)\n",
    "    # Saving the features and labels to csv files\n",
    "    train.to_csv('data/folds/training_folds/fold{}/train_fold_{}_features.csv'.format(i,i), index=False)\n",
    "    test.to_csv('data/folds/validation_folds/fold{}/test_fold_{}_features.csv'.format(i,i), index=False)\n",
    "    np.savetxt('data/folds/training_folds/fold{}/train_fold_{}_labels.csv'.format(i,i), train_labels, delimiter=',', fmt='%d')\n",
    "    np.savetxt('data/folds/validation_folds/fold{}/test_fold_{}_labels.csv'.format(i,i), test_labels, delimiter=',', fmt='%d')\n",
    "    print(\"Fold # {} features and lables saved to csv files in folder: data/fold{}\".format(i,i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82db0f28",
   "metadata": {},
   "source": [
    "### Prepare data in graphs form for use by GNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4867fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82, 2412)\n",
      "(82, 82)\n",
      "(82, 82)\n",
      "Fold : 1 done\n",
      "(82, 2412)\n",
      "(82, 82)\n",
      "(82, 82)\n",
      "Fold : 2 done\n",
      "(82, 2412)\n",
      "(82, 82)\n",
      "(82, 82)\n",
      "Fold : 3 done\n",
      "(83, 2412)\n"
     ]
    }
   ],
   "source": [
    "# Create a matrix of normalized euclidean distance from df_data for patients\n",
    "# For Train Folds\n",
    "for i in range(1, 6):\n",
    "    train_df_data = pd.read_csv('data/folds/training_folds/fold{}/train_fold_{}_features.csv'.format(i,i))    \n",
    "    train_patient_dist_matrix = np.zeros((train_df_data.shape[0], train_df_data.shape[0]))\n",
    "    print(train_df_data.shape)\n",
    "    for j in range(train_df_data.shape[0]):\n",
    "        for k in range(train_df_data.shape[0]):\n",
    "            train_patient_dist_matrix[j][k] = np.linalg.norm(train_df_data.iloc[j] - train_df_data.iloc[k])\n",
    "    print(train_patient_dist_matrix.shape)\n",
    "    # normalize the matrix to [0, 1]\n",
    "    train_patient_dist_matrix = (train_patient_dist_matrix - train_patient_dist_matrix.min()) / (train_patient_dist_matrix.max() - train_patient_dist_matrix.min())\n",
    "    print(train_patient_dist_matrix.shape)\n",
    "    # print(train_patient_dist_matrix)\n",
    "    np.savetxt('data/folds/training_folds/fold{}/fold_{}_train_patient_dist_matrix.csv'.format(i,i), train_patient_dist_matrix, delimiter=',', fmt='%f')\n",
    "    # print(\"Fold # {} patient distance matrix saved to csv files in folder: data/fold{}\".format(i,i))\n",
    "    print('Fold :', i, 'done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "423f5b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 2412)\n",
      "(21, 21)\n",
      "Fold : 1 done\n",
      "(21, 2412)\n",
      "(21, 21)\n",
      "Fold : 2 done\n",
      "(21, 2412)\n",
      "(21, 21)\n",
      "Fold : 3 done\n",
      "(20, 2412)\n",
      "(20, 20)\n",
      "Fold : 4 done\n",
      "(20, 2412)\n",
      "(20, 20)\n",
      "Fold : 5 done\n"
     ]
    }
   ],
   "source": [
    "# Create a matrix of normalized euclidean distance from df_data for patients\n",
    "# For Test Folds\n",
    "for i in range(1, 6):\n",
    "    test_df_data = pd.read_csv('data/folds/validation_folds/fold{}/test_fold_{}_features.csv'.format(i,i))    \n",
    "    test_patient_dist_matrix = np.zeros((test_df_data.shape[0], test_df_data.shape[0]))\n",
    "    print(test_df_data.shape)\n",
    "    for j in range(test_df_data.shape[0]):\n",
    "        for k in range(test_df_data.shape[0]):\n",
    "            test_patient_dist_matrix[j][k] = np.linalg.norm(test_df_data.iloc[j] - test_df_data.iloc[k])\n",
    "    print(test_patient_dist_matrix.shape)\n",
    "    # normalize the matrix to [0, 1]\n",
    "    test_patient_dist_matrix = (test_patient_dist_matrix - test_patient_dist_matrix.min()) / (test_patient_dist_matrix.max() - test_patient_dist_matrix.min())\n",
    "    # print(test_patient_dist_matrix)\n",
    "    np.savetxt('data/folds/validation_folds/fold{}/fold_{}_test_patient_dist_matrix.csv'.format(i,i), test_patient_dist_matrix, delimiter=',', fmt='%f')\n",
    "    # print(\"Fold # {} patient distance matrix saved to csv files in folder: data/fold{}\".format(i,i))\n",
    "    print('Fold :', i, 'done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "020889e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************** Starting Fold : 1 ********************\n",
      "Loading folder: data/folds/training_folds/fold1\n",
      "Adjacency matrix shape: (82, 82)  Node features shape: (82, 2412)  Labels shape: (82, 1)\n",
      "num_nodes: 82 , num_features: 2412 , num_labels: 82 , num_classes: 3\n",
      "zero_count: 82\n",
      "Edge indices shape: (2, 6642)  Edge attributes shape: (6642,)\n",
      "****************** Starting Fold : 2 ********************\n",
      "Loading folder: data/folds/training_folds/fold2\n",
      "Adjacency matrix shape: (82, 82)  Node features shape: (82, 2412)  Labels shape: (82, 1)\n",
      "num_nodes: 82 , num_features: 2412 , num_labels: 82 , num_classes: 3\n",
      "zero_count: 82\n",
      "Edge indices shape: (2, 6642)  Edge attributes shape: (6642,)\n",
      "****************** Starting Fold : 3 ********************\n",
      "Loading folder: data/folds/training_folds/fold3\n",
      "Adjacency matrix shape: (82, 82)  Node features shape: (82, 2412)  Labels shape: (82, 1)\n",
      "num_nodes: 82 , num_features: 2412 , num_labels: 82 , num_classes: 3\n",
      "zero_count: 82\n",
      "Edge indices shape: (2, 6642)  Edge attributes shape: (6642,)\n",
      "****************** Starting Fold : 4 ********************\n",
      "Loading folder: data/folds/training_folds/fold4\n",
      "Adjacency matrix shape: (83, 83)  Node features shape: (83, 2412)  Labels shape: (83, 1)\n",
      "num_nodes: 83 , num_features: 2412 , num_labels: 83 , num_classes: 3\n",
      "zero_count: 83\n",
      "Edge indices shape: (2, 6806)  Edge attributes shape: (6806,)\n",
      "****************** Starting Fold : 5 ********************\n",
      "Loading folder: data/folds/training_folds/fold5\n",
      "Adjacency matrix shape: (83, 83)  Node features shape: (83, 2412)  Labels shape: (83, 1)\n",
      "num_nodes: 83 , num_features: 2412 , num_labels: 83 , num_classes: 3\n",
      "zero_count: 83\n",
      "Edge indices shape: (2, 6806)  Edge attributes shape: (6806,)\n"
     ]
    }
   ],
   "source": [
    "# generate edge indices and edge attributes for each training fold\n",
    "for k in range(1, 6):\n",
    "    print('****************** Starting Fold :', k, '********************')\n",
    "    print('Loading folder: data/folds/training_folds/fold{}'.format(k)) \n",
    "    labels = pd.read_csv('data/folds/training_folds/fold{}/train_fold_{}_labels.csv'.format(k,k), header=None).values\n",
    "    node_features = pd.read_csv('data/folds/training_folds/fold{}/train_fold_{}_features.csv'.format(k,k), skiprows=1, header=None).values\n",
    "    adj = pd.read_csv('data/folds/training_folds/fold{}/fold_{}_train_patient_dist_matrix.csv'.format(k,k), header=None).values   # Adjacency matrix (num_nodes, num_nodes)\n",
    "    print('Adjacency matrix shape:', adj.shape, ' Node features shape:', node_features.shape, ' Labels shape:', labels.shape)\n",
    "\n",
    "    num_nodes = adj.shape[0]\n",
    "    num_features = node_features.shape[1]\n",
    "    num_labels = labels.shape[0]\n",
    "    num_classes = 3\n",
    "    print('num_nodes:',num_nodes, ', num_features:',num_features, ', num_labels:',num_labels, ', num_classes:',num_classes)\n",
    "    # count the zero elements in the adjacency matrix\n",
    "    zero_count = 0\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if adj[i][j] == 0:\n",
    "                zero_count += 1\n",
    "    print('zero_count:',zero_count)\n",
    "\n",
    "    # Generate edge indices and edge attributes\n",
    "    edge_indices = []\n",
    "    edge_attributes = []\n",
    "    for l in range(num_nodes):\n",
    "        for m in range(num_nodes):\n",
    "            if adj[l][m] > 0:\n",
    "                edge_indices.append([l,m])\n",
    "                edge_attributes.append(adj[l][m])\n",
    "    edge_indices = np.array(edge_indices).T  # (2, num_edges) where num_edges = num_nodes * num_nodes - zero_count\n",
    "    edge_attributes = np.array(edge_attributes)\n",
    "    print('Edge indices shape:', edge_indices.shape, ' Edge attributes shape:', edge_attributes.shape)\n",
    "    # print first 6 instances of edge_index and edge_attr\n",
    "    # print('edge_index:',edge_indices[:,:6])\n",
    "    # print('edge_attr:',edge_attributes[:6])\n",
    "\n",
    "    # check validity of edge_index and edge_attr\n",
    "    for a in range(edge_indices.shape[1]):\n",
    "        if edge_indices[0][a] == edge_indices[1][a]:\n",
    "            print('Self loop found at index:', a)\n",
    "    for b in range(edge_attributes.shape[0]):\n",
    "        if edge_attributes[b] == 0:\n",
    "            print('Zero edge attribute found at index:', b)\n",
    "    # save edge indices and edge attributes to csv files\n",
    "    np.savetxt('data/folds/training_folds/fold{}/fold_{}_train_edge_indices.csv'.format(k,k), edge_indices, delimiter=',', fmt='%d')\n",
    "    np.savetxt('data/folds/training_folds/fold{}/fold_{}_train_edge_attributes.csv'.format(k,k), edge_attributes, delimiter=',', fmt='%f')\n",
    "    \n",
    "# # create node coordinates\n",
    "# node_coordinates = np.zeros((num_nodes, 2))\n",
    "# for i in range(num_nodes):\n",
    "#     node_coordinates[i][0] = i\n",
    "#     node_coordinates[i][1] = i\n",
    "# print('node_coordinates:',node_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a4c9a95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************** Starting Fold : 1 ********************\n",
      "Loading folder: data/folds/validation_folds/fold1\n",
      "Adjacency matrix shape: (21, 21)  Node features shape: (21, 2412)  Labels shape: (21, 1)\n",
      "num_nodes: 21 , num_features: 2412 , num_labels: 21 , num_classes: 3\n",
      "zero_count: 21\n",
      "Edge indices shape: (2, 420)  Edge attributes shape: (420,)\n",
      "****************** Starting Fold : 2 ********************\n",
      "Loading folder: data/folds/validation_folds/fold2\n",
      "Adjacency matrix shape: (21, 21)  Node features shape: (21, 2412)  Labels shape: (21, 1)\n",
      "num_nodes: 21 , num_features: 2412 , num_labels: 21 , num_classes: 3\n",
      "zero_count: 21\n",
      "Edge indices shape: (2, 420)  Edge attributes shape: (420,)\n",
      "****************** Starting Fold : 3 ********************\n",
      "Loading folder: data/folds/validation_folds/fold3\n",
      "Adjacency matrix shape: (21, 21)  Node features shape: (21, 2412)  Labels shape: (21, 1)\n",
      "num_nodes: 21 , num_features: 2412 , num_labels: 21 , num_classes: 3\n",
      "zero_count: 21\n",
      "Edge indices shape: (2, 420)  Edge attributes shape: (420,)\n",
      "****************** Starting Fold : 4 ********************\n",
      "Loading folder: data/folds/validation_folds/fold4\n",
      "Adjacency matrix shape: (20, 20)  Node features shape: (20, 2412)  Labels shape: (20, 1)\n",
      "num_nodes: 20 , num_features: 2412 , num_labels: 20 , num_classes: 3\n",
      "zero_count: 20\n",
      "Edge indices shape: (2, 380)  Edge attributes shape: (380,)\n",
      "****************** Starting Fold : 5 ********************\n",
      "Loading folder: data/folds/validation_folds/fold5\n",
      "Adjacency matrix shape: (20, 20)  Node features shape: (20, 2412)  Labels shape: (20, 1)\n",
      "num_nodes: 20 , num_features: 2412 , num_labels: 20 , num_classes: 3\n",
      "zero_count: 20\n",
      "Edge indices shape: (2, 380)  Edge attributes shape: (380,)\n"
     ]
    }
   ],
   "source": [
    "# generate edge indices and edge attributes for each validation fold\n",
    "for k in range(1, 6):\n",
    "    print('****************** Starting Fold :', k, '********************')\n",
    "    print('Loading folder: data/folds/validation_folds/fold{}'.format(k)) \n",
    "    labels = pd.read_csv('data/folds/validation_folds/fold{}/test_fold_{}_labels.csv'.format(k,k), header=None).values\n",
    "    node_features = pd.read_csv('data/folds/validation_folds/fold{}/test_fold_{}_features.csv'.format(k,k), skiprows=1, header=None).values\n",
    "    adj = pd.read_csv('data/folds/validation_folds/fold{}/fold_{}_test_patient_dist_matrix.csv'.format(k,k), header=None).values   # Adjacency matrix (num_nodes, num_nodes)\n",
    "    print('Adjacency matrix shape:', adj.shape, ' Node features shape:', node_features.shape, ' Labels shape:', labels.shape)\n",
    "\n",
    "    num_nodes = adj.shape[0]\n",
    "    num_features = node_features.shape[1]\n",
    "    num_labels = labels.shape[0]\n",
    "    num_classes = 3\n",
    "    print('num_nodes:',num_nodes, ', num_features:',num_features, ', num_labels:',num_labels, ', num_classes:',num_classes)\n",
    "    # count the zero elements in the adjacency matrix\n",
    "    zero_count = 0\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if adj[i][j] == 0:\n",
    "                zero_count += 1\n",
    "    print('zero_count:',zero_count)\n",
    "\n",
    "    # Generate edge indices and edge attributes\n",
    "    edge_indices = []\n",
    "    edge_attributes = []\n",
    "    for l in range(num_nodes):\n",
    "        for m in range(num_nodes):\n",
    "            if adj[l][m] > 0:\n",
    "                edge_indices.append([l,m])\n",
    "                edge_attributes.append(adj[l][m])\n",
    "    edge_indices = np.array(edge_indices).T  # (2, num_edges) where num_edges = num_nodes * num_nodes - zero_count\n",
    "    edge_attributes = np.array(edge_attributes)\n",
    "    print('Edge indices shape:', edge_indices.shape, ' Edge attributes shape:', edge_attributes.shape)\n",
    "    # print first 6 instances of edge_index and edge_attr\n",
    "    # print('edge_index:',edge_indices[:,:6])\n",
    "    # print('edge_attr:',edge_attributes[:6])\n",
    "\n",
    "    # check validity of edge_index and edge_attr\n",
    "    for a in range(edge_indices.shape[1]):\n",
    "        if edge_indices[0][a] == edge_indices[1][a]:\n",
    "            print('Self loop found at index:', a)\n",
    "    for b in range(edge_attributes.shape[0]):\n",
    "        if edge_attributes[b] == 0:\n",
    "            print('Zero edge attribute found at index:', b)\n",
    "    # save edge indices and edge attributes to csv files\n",
    "    np.savetxt('data/folds/validation_folds/fold{}/fold_{}_test_edge_indices.csv'.format(k,k), edge_indices, delimiter=',', fmt='%d')\n",
    "    np.savetxt('data/folds/validation_folds/fold{}/fold_{}_test_edge_attributes.csv'.format(k,k), edge_attributes, delimiter=',', fmt='%f')\n",
    "    \n",
    "# # create node coordinates\n",
    "# node_coordinates = np.zeros((num_nodes, 2))\n",
    "# for i in range(num_nodes):\n",
    "#     node_coordinates[i][0] = i\n",
    "#     node_coordinates[i][1] = i\n",
    "# print('node_coordinates:',node_coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97fef4a",
   "metadata": {},
   "source": [
    "# Pytorch Geomteric & GNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb8eaed",
   "metadata": {},
   "source": [
    "### Create custom data loader for torch.geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "797a6131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages from torch_geometric\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU\n",
    "\n",
    "from torch_geometric.nn import GraphConv, GINConv, TopKPooling\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.data import DataListLoader\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.data import NeighborSampler\n",
    "from torch_geometric.data import ClusterData\n",
    "from torch_geometric.data import ClusterLoader\n",
    "from torch_geometric.data import ClusterLoader\n",
    "from torch_geometric.data import ClusterData\n",
    "from torch_geometric.data import download_url, extract_zip\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "# from torch_geometric.loader import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0abffae",
   "metadata": {},
   "source": [
    "### GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64a1c661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GNN model, GIN\n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, num_feature, num_class, nhid):\n",
    "        super(GINTopK, self).__init__()\n",
    "        self.conv1 = GINConv(Seq(Lin(num_feature, nhid), ReLU(), Lin(nhid, nhid)))\n",
    "        self.conv2 = GINConv(Seq(Lin(nhid, nhid), ReLU(), Lin(nhid, nhid)))\n",
    "        self.conv3 = GINConv(Seq(Lin(nhid, nhid), ReLU(), Lin(nhid, nhid)))\n",
    "        self.conv4 = GINConv(Seq(Lin(nhid, nhid), ReLU(), Lin(nhid, nhid)))\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(nhid, nhid)\n",
    "        self.lin2 = torch.nn.Linear(nhid, 82)\n",
    "        self.lin3 = torch.nn.Linear(82, num_class)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index.t(), data.batch\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        y1 = F.log_softmax(self.lin3(x), dim=-1) # Final GNN output\n",
    "        y2 = torch.sigmoid(self.lin3(x)) # Classifier output\n",
    "        return y1, y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1eed052",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can use another GNN model for training. GCN was the original seminal work.\n",
    "\n",
    "# Uncomment the lines below to use this model\n",
    "\n",
    "# import torch\n",
    "# from torch.nn import Linear\n",
    "# from torch_geometric.nn import GCNConv\n",
    "\n",
    "# class GCN(torch.nn.Module):\n",
    "#     def __init__(self, num_feature, num_class, nhid):\n",
    "#         super(GCN, self).__init__()\n",
    "#         torch.manual_seed(12345)\n",
    "#         self.conv1 = GCNConv(num_feature, nhid)\n",
    "#         self.conv2 = GCNConv(nhid, nhid)\n",
    "#         self.conv3 = GCNConv(nhid, nhid)\n",
    "#         self.classifier = Linear(nhid, num_class)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         h = self.conv1(x, edge_index)\n",
    "#         h = h.tanh()\n",
    "#         h = self.conv2(h, edge_index)\n",
    "#         h = h.tanh()\n",
    "#         h = self.conv3(h, edge_index)\n",
    "#         h = h.tanh()    # Final GNN embedding space.\n",
    "#         out = self.classifier(h)\n",
    "#         return out, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a10d593",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d0f960ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "learning_rate = 5e-4\n",
    "weight_decay = 1e-4\n",
    "nhid = 512\n",
    "pooling_ratio = 0.5\n",
    "epochs = 500\n",
    "num_class = 3\n",
    "# early_stopping = args.early_stopping\n",
    "num_layers = 4\n",
    "model_name = \"gin\"\n",
    "runs = 1\n",
    "fold = 5\n",
    "\n",
    "train_loss = np.zeros((1,epochs),dtype=float)\n",
    "val_acc = np.zeros((1,epochs))\n",
    "val_loss = np.zeros((1,epochs))\n",
    "val_pred = np.zeros(1)\n",
    "val_out = np.zeros((1,num_class))\n",
    "groud_truth = np.zeros((1,num_class))\n",
    "test_acc_c = np.zeros(1)\n",
    "test_loss_c = np.zeros(1)\n",
    "test_pred_c = np.zeros(1)\n",
    "test_out_c = np.zeros((1,num_class)) \n",
    "groud_truth_c = np.zeros((1,num_class))\n",
    "test_acc_p = np.zeros(1)\n",
    "min_loss = 1e10*np.ones(1)\n",
    "\n",
    "# early_stopping = epochs\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241785fe",
   "metadata": {},
   "source": [
    "### Data class and Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb0dac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a torch geometric dataset for each fold of trainig and validation sets\n",
    "\n",
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, fold=None):\n",
    "        super(MyOwnDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "    \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['fold_{fold}_edge_indices.csv', 'fold_{fold}_edge_attributes.csv', 'fold_{fold}_features.csv', 'fold_{fold}_labels.csv']\n",
    "            \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        # load and preprocess data for Lung cancer\n",
    "        ld_edge_index = \"\"\n",
    "        ld_edge_attr = \"\"\n",
    "        ld_feature = \"\"\n",
    "        ld_label = \"\"\n",
    "\n",
    "        print(f'Looking into directory: {self.raw_dir} ...')\n",
    "        for _root, _dirs, _files in os.walk(self.raw_dir):\n",
    "            for _file in _files:\n",
    "                print(f'Loading File : {_file}')\n",
    "                if(\"edge_indices\" in _file):\n",
    "                    ld_edge_index = os.path.join(_root, _file)\n",
    "                if(\"edge_attributes\" in _file):\n",
    "                    ld_edge_attr = os.path.join(_root, _file)\n",
    "                elif(\"features\" in _file):\n",
    "                    ld_feature = os.path.join(_root, _file)\n",
    "                elif(\"labels\" in _file):\n",
    "                    ld_label = os.path.join(_root, _file)\n",
    "        \n",
    "        # load feature from .csv file without header\n",
    "        node_feature = pd.read_csv(ld_feature, skiprows=1, header=None, sep=',')\n",
    "        node_feature.index += 1\n",
    "        print('Node_Features :', node_feature.shape)\n",
    "        # load edge_attr from .csv file\n",
    "        edge_attr = pd.read_csv(ld_edge_attr, header=None, sep=',', names=['edge_attr'])\n",
    "        edge_attr.index += 1\n",
    "        print('Edge_attr :', edge_attr.shape)\n",
    "        # load edge_index from .csv files\n",
    "        edge_index = pd.read_csv(ld_edge_index, header=None, sep=',')\n",
    "        edge_index.index += 1\n",
    "        print('Edge_index :',edge_index.shape)\n",
    "        # load label from .csv file\n",
    "        label = pd.read_csv(ld_label, header=None, sep=',', names=['label'])\n",
    "        label.index += 1\n",
    "        print('Label :', label.shape)\n",
    "\n",
    "        data_list = list()\n",
    "        # Get values of the features\n",
    "        N_features = node_feature.values\n",
    "        print('*********', N_features.shape)\n",
    "        E_attrs = edge_attr.values\n",
    "        E_indices = edge_index.values\n",
    "        N_labels = label.values\n",
    "        # Convert DataFrames to tensors     \n",
    "        x = torch.tensor(N_features, dtype=torch.float)\n",
    "        edge_attrs = torch.tensor(E_attrs, dtype=torch.float)\n",
    "        edge_indices = torch.tensor(E_indices, dtype=torch.long)\n",
    "        edge_indices = edge_indices.t().contiguous()\n",
    "        N_labels = np.asarray([N_labels])\n",
    "        y = (torch.tensor(N_labels, dtype=torch.long)).shape[1]\n",
    "        \n",
    "        graph = Data(x=x, edge_index=edge_indices, edge_attr=edge_attrs, y=y.squeeze())\n",
    "        data_list.append(graph)\n",
    "\n",
    "        # Apply the functions specified in pre_filter and pre_transform\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        # Store the processed data\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517693b4",
   "metadata": {},
   "source": [
    "### Training and Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44b6c759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 1552351744.00, Val loss: 15087959.00\n",
      "Epoch: 1, Train loss: 3813244928.00, Val loss: 12781385.00\n",
      "Epoch: 2, Train loss: 2784259072.00, Val loss: 5319687.50\n",
      "Epoch: 3, Train loss: 1302889344.00, Val loss: 1655815.38\n",
      "Epoch: 4, Train loss: 703488640.00, Val loss: 2735068.50\n",
      "Epoch: 5, Train loss: 522768544.00, Val loss: 1718276.62\n",
      "Epoch: 6, Train loss: 346663776.00, Val loss: 1043917.44\n",
      "Epoch: 7, Train loss: 284640512.00, Val loss: 1114080.88\n",
      "Epoch: 8, Train loss: 224199328.00, Val loss: 1013440.75\n",
      "Epoch: 9, Train loss: 174285856.00, Val loss: 721229.06\n",
      "Epoch: 10, Train loss: 218255888.00, Val loss: 494568.38\n",
      "Epoch: 11, Train loss: 109281232.00, Val loss: 343111.84\n",
      "Epoch: 12, Train loss: 110000400.00, Val loss: 393711.16\n",
      "Epoch: 13, Train loss: 80947832.00, Val loss: 476545.06\n",
      "Epoch: 14, Train loss: 96415544.00, Val loss: 522386.09\n",
      "Epoch: 15, Train loss: 88271264.00, Val loss: 283158.75\n",
      "Epoch: 16, Train loss: 67705592.00, Val loss: 298251.09\n",
      "Epoch: 17, Train loss: 56093932.00, Val loss: 282770.31\n",
      "Epoch: 18, Train loss: 49698952.00, Val loss: 229716.50\n",
      "Epoch: 19, Train loss: 38358224.00, Val loss: 196825.27\n",
      "Epoch: 20, Train loss: 32114186.00, Val loss: 105870.09\n",
      "Epoch: 21, Train loss: 27951472.00, Val loss: 230750.14\n",
      "Epoch: 22, Train loss: 32752310.00, Val loss: 157383.67\n",
      "Epoch: 23, Train loss: 24799700.00, Val loss: 84065.73\n",
      "Epoch: 24, Train loss: 24558624.00, Val loss: 65016.34\n",
      "Epoch: 25, Train loss: 20632764.00, Val loss: 66283.01\n",
      "Epoch: 26, Train loss: 15778663.00, Val loss: 60790.67\n",
      "Epoch: 27, Train loss: 12532732.00, Val loss: 50757.84\n",
      "Epoch: 28, Train loss: 13320003.00, Val loss: 73618.09\n",
      "Epoch: 29, Train loss: 13327396.00, Val loss: 50384.86\n",
      "Epoch: 30, Train loss: 11499568.00, Val loss: 63020.05\n",
      "Epoch: 31, Train loss: 12960954.00, Val loss: 34580.81\n",
      "Epoch: 32, Train loss: 8652887.00, Val loss: 32754.82\n",
      "Epoch: 33, Train loss: 9816231.00, Val loss: 30113.59\n",
      "Epoch: 34, Train loss: 7717903.00, Val loss: 21064.70\n",
      "Epoch: 35, Train loss: 6435086.00, Val loss: 26386.32\n",
      "Epoch: 36, Train loss: 6360796.00, Val loss: 27552.02\n",
      "Epoch: 37, Train loss: 5038058.00, Val loss: 19140.61\n",
      "Epoch: 38, Train loss: 5937891.00, Val loss: 15966.09\n",
      "Epoch: 39, Train loss: 4341237.50, Val loss: 15550.56\n",
      "Epoch: 40, Train loss: 2826978.00, Val loss: 10866.21\n",
      "Epoch: 41, Train loss: 3060637.00, Val loss: 11899.63\n",
      "Epoch: 42, Train loss: 3116609.50, Val loss: 18162.46\n",
      "Epoch: 43, Train loss: 2535146.25, Val loss: 11095.47\n",
      "Epoch: 44, Train loss: 2623736.00, Val loss: 12064.24\n",
      "Epoch: 45, Train loss: 2024459.38, Val loss: 6476.67\n",
      "Epoch: 46, Train loss: 1981416.62, Val loss: 4922.84\n",
      "Epoch: 47, Train loss: 1708345.12, Val loss: 5837.09\n",
      "Epoch: 48, Train loss: 1879321.12, Val loss: 6069.41\n",
      "Epoch: 49, Train loss: 1492426.00, Val loss: 9311.48\n",
      "Epoch: 50, Train loss: 1446239.25, Val loss: 8441.83\n",
      "Epoch: 51, Train loss: 1871748.25, Val loss: 7362.25\n",
      "Epoch: 52, Train loss: 1463124.25, Val loss: 6051.25\n",
      "Epoch: 53, Train loss: 1372027.38, Val loss: 9864.43\n",
      "Epoch: 54, Train loss: 1493153.50, Val loss: 5738.75\n",
      "Epoch: 55, Train loss: 1530136.50, Val loss: 6306.53\n",
      "Epoch: 56, Train loss: 1647360.62, Val loss: 7046.90\n",
      "Epoch: 57, Train loss: 1579202.88, Val loss: 6300.10\n",
      "Epoch: 58, Train loss: 1403828.62, Val loss: 4790.94\n",
      "Epoch: 59, Train loss: 1249081.12, Val loss: 3078.13\n",
      "Epoch: 60, Train loss: 1426173.25, Val loss: 4334.80\n",
      "Epoch: 61, Train loss: 1151448.50, Val loss: 4413.48\n",
      "Epoch: 62, Train loss: 860673.88, Val loss: 2800.42\n",
      "Epoch: 63, Train loss: 899257.56, Val loss: 3235.22\n",
      "Epoch: 64, Train loss: 752291.19, Val loss: 4987.58\n",
      "Epoch: 65, Train loss: 528405.69, Val loss: 3548.21\n",
      "Epoch: 66, Train loss: 747524.12, Val loss: 4349.89\n",
      "Epoch: 67, Train loss: 549091.38, Val loss: 2710.97\n",
      "Epoch: 68, Train loss: 647381.81, Val loss: 2852.53\n",
      "Epoch: 69, Train loss: 433859.75, Val loss: 2293.45\n",
      "Epoch: 70, Train loss: 663425.62, Val loss: 1419.87\n",
      "Epoch: 71, Train loss: 563570.19, Val loss: 2141.17\n",
      "Epoch: 72, Train loss: 559819.69, Val loss: 2925.01\n",
      "Epoch: 73, Train loss: 532464.31, Val loss: 1403.16\n",
      "Epoch: 74, Train loss: 413976.84, Val loss: 2328.37\n",
      "Epoch: 75, Train loss: 490941.28, Val loss: 1866.28\n",
      "Epoch: 76, Train loss: 431661.47, Val loss: 1755.45\n",
      "Epoch: 77, Train loss: 539762.88, Val loss: 1339.39\n",
      "Epoch: 78, Train loss: 433371.81, Val loss: 2600.07\n",
      "Epoch: 79, Train loss: 380400.84, Val loss: 733.29\n",
      "Epoch: 80, Train loss: 455833.12, Val loss: 1485.29\n",
      "Epoch: 81, Train loss: 283999.34, Val loss: 649.21\n",
      "Epoch: 82, Train loss: 435057.31, Val loss: 1109.78\n",
      "Epoch: 83, Train loss: 316012.31, Val loss: 1587.26\n",
      "Epoch: 84, Train loss: 275166.94, Val loss: 1109.42\n",
      "Epoch: 85, Train loss: 342385.91, Val loss: 1818.87\n",
      "Epoch: 86, Train loss: 234652.97, Val loss: 662.44\n",
      "Epoch: 87, Train loss: 218277.39, Val loss: 695.10\n",
      "Epoch: 88, Train loss: 140229.44, Val loss: 1120.67\n",
      "Epoch: 89, Train loss: 172871.78, Val loss: 895.44\n",
      "Epoch: 90, Train loss: 207613.92, Val loss: 697.81\n",
      "Epoch: 91, Train loss: 193090.81, Val loss: 992.43\n",
      "Epoch: 92, Train loss: 135859.42, Val loss: 763.38\n",
      "Epoch: 93, Train loss: 146893.48, Val loss: 816.99\n",
      "Epoch: 94, Train loss: 109074.17, Val loss: 360.00\n",
      "Epoch: 95, Train loss: 159519.45, Val loss: 1164.46\n",
      "Epoch: 96, Train loss: 128758.15, Val loss: 339.18\n",
      "Epoch: 97, Train loss: 154043.06, Val loss: 273.20\n",
      "Epoch: 98, Train loss: 91051.91, Val loss: 102.59\n",
      "Epoch: 99, Train loss: 66913.59, Val loss: 405.73\n",
      "Epoch: 100, Train loss: 70673.80, Val loss: 44.40\n",
      "Epoch: 101, Train loss: 46665.14, Val loss: 41.11\n",
      "Epoch: 102, Train loss: 35207.04, Val loss: 241.35\n",
      "Epoch: 103, Train loss: 24047.07, Val loss: 40.05\n",
      "Epoch: 104, Train loss: 52840.96, Val loss: 142.73\n",
      "Epoch: 105, Train loss: 23264.37, Val loss: 91.60\n",
      "Epoch: 106, Train loss: 57151.20, Val loss: 71.16\n",
      "Epoch: 107, Train loss: 37091.13, Val loss: 131.06\n",
      "Epoch: 108, Train loss: 17094.04, Val loss: 80.81\n",
      "Epoch: 109, Train loss: 34010.72, Val loss: 177.52\n",
      "Epoch: 110, Train loss: 9928.31, Val loss: 68.28\n",
      "Epoch: 111, Train loss: 29974.33, Val loss: 58.74\n",
      "Epoch: 112, Train loss: 13887.93, Val loss: 30.04\n",
      "Epoch: 113, Train loss: 25025.93, Val loss: 37.48\n",
      "Epoch: 114, Train loss: 32148.99, Val loss: 197.08\n",
      "Epoch: 115, Train loss: 10482.85, Val loss: 203.79\n",
      "Epoch: 116, Train loss: 6373.08, Val loss: 36.33\n",
      "Epoch: 117, Train loss: 16851.64, Val loss: 98.71\n",
      "Epoch: 118, Train loss: 8133.85, Val loss: 45.05\n",
      "Epoch: 119, Train loss: 15033.28, Val loss: 17.97\n",
      "Epoch: 120, Train loss: 11507.82, Val loss: 14.79\n",
      "Epoch: 121, Train loss: 4106.44, Val loss: 137.60\n",
      "Epoch: 122, Train loss: 22589.85, Val loss: 5.10\n",
      "Epoch: 123, Train loss: 17194.77, Val loss: 461.59\n",
      "Epoch: 124, Train loss: 15215.58, Val loss: 1.09\n",
      "Epoch: 125, Train loss: 1970.22, Val loss: 28.90\n",
      "Epoch: 126, Train loss: 26926.11, Val loss: 1.04\n",
      "Epoch: 127, Train loss: 2354.83, Val loss: 14.25\n",
      "Epoch: 128, Train loss: 7572.60, Val loss: 1.09\n",
      "Epoch: 129, Train loss: 10752.32, Val loss: 48.59\n",
      "Epoch: 130, Train loss: 8380.52, Val loss: 83.55\n",
      "Epoch: 131, Train loss: 8672.19, Val loss: 1.09\n",
      "Epoch: 132, Train loss: 13416.37, Val loss: 0.99\n",
      "Epoch: 133, Train loss: 4748.22, Val loss: 1.09\n",
      "Epoch: 134, Train loss: 2794.40, Val loss: 1.09\n",
      "Epoch: 135, Train loss: 4024.43, Val loss: 1.04\n",
      "Epoch: 136, Train loss: 6220.01, Val loss: 1.09\n",
      "Epoch: 137, Train loss: 522.61, Val loss: 1.05\n",
      "Epoch: 138, Train loss: 514.91, Val loss: 38.54\n",
      "Epoch: 139, Train loss: 317.95, Val loss: 1.09\n",
      "Epoch: 140, Train loss: 261.31, Val loss: 90.26\n",
      "Epoch: 141, Train loss: 9830.76, Val loss: 1.04\n",
      "Epoch: 142, Train loss: 1365.16, Val loss: 2.41\n",
      "Epoch: 143, Train loss: 1470.77, Val loss: 14.41\n",
      "Epoch: 144, Train loss: 12556.39, Val loss: 151.79\n",
      "Epoch: 145, Train loss: 10061.06, Val loss: 5.82\n",
      "Epoch: 146, Train loss: 3954.74, Val loss: 1.09\n",
      "Epoch: 147, Train loss: 2021.53, Val loss: 1.09\n",
      "Epoch: 148, Train loss: 880.90, Val loss: 1.09\n",
      "Epoch: 149, Train loss: 3469.10, Val loss: 1.04\n",
      "Epoch: 150, Train loss: 488.80, Val loss: 1.09\n",
      "Epoch: 151, Train loss: 4074.94, Val loss: 1.04\n",
      "Epoch: 152, Train loss: 11307.02, Val loss: 0.99\n",
      "Epoch: 153, Train loss: 4137.01, Val loss: 1.04\n",
      "Epoch: 154, Train loss: 1.02, Val loss: 13.89\n",
      "Epoch: 155, Train loss: 6042.12, Val loss: 1.09\n",
      "Epoch: 156, Train loss: 97.93, Val loss: 1.09\n",
      "Epoch: 157, Train loss: 451.29, Val loss: 21.16\n",
      "Epoch: 158, Train loss: 2992.22, Val loss: 11.93\n",
      "Epoch: 159, Train loss: 771.54, Val loss: 1.04\n",
      "Epoch: 160, Train loss: 1.04, Val loss: 1.04\n",
      "Epoch: 161, Train loss: 3209.14, Val loss: 1.09\n",
      "Epoch: 162, Train loss: 1017.17, Val loss: 1.09\n",
      "Epoch: 163, Train loss: 7187.25, Val loss: 1.09\n",
      "Epoch: 164, Train loss: 3338.74, Val loss: 1.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 165, Train loss: 158.26, Val loss: 1.09\n",
      "Epoch: 166, Train loss: 6385.75, Val loss: 1.09\n",
      "Epoch: 167, Train loss: 1.09, Val loss: 1.09\n",
      "Epoch: 168, Train loss: 1031.32, Val loss: 21.49\n",
      "Epoch: 169, Train loss: 4394.07, Val loss: 136.25\n",
      "Epoch: 170, Train loss: 2374.08, Val loss: 1.09\n",
      "Epoch: 171, Train loss: 10159.42, Val loss: 2.59\n",
      "Epoch: 172, Train loss: 931.90, Val loss: 1.09\n",
      "Epoch: 173, Train loss: 68.39, Val loss: 44.62\n",
      "Epoch: 174, Train loss: 2766.94, Val loss: 1.09\n",
      "Epoch: 175, Train loss: 1.06, Val loss: 1.09\n",
      "Epoch: 176, Train loss: 68.20, Val loss: 1.04\n",
      "Epoch: 177, Train loss: 10689.01, Val loss: 13.44\n",
      "Epoch: 178, Train loss: 3345.00, Val loss: 1.09\n",
      "Epoch: 179, Train loss: 2473.78, Val loss: 1.09\n",
      "Epoch: 180, Train loss: 1.09, Val loss: 1.09\n",
      "Epoch: 181, Train loss: 819.35, Val loss: 1.09\n",
      "Epoch: 182, Train loss: 258.71, Val loss: 86.70\n",
      "Epoch: 183, Train loss: 8749.30, Val loss: 83.12\n",
      "Epoch: 184, Train loss: 10159.64, Val loss: 29.86\n",
      "Epoch: 185, Train loss: 2555.91, Val loss: 6.39\n",
      "Epoch: 186, Train loss: 2551.93, Val loss: 1.04\n",
      "Epoch: 187, Train loss: 1901.51, Val loss: 1.04\n",
      "Epoch: 188, Train loss: 1751.78, Val loss: 1.09\n",
      "Epoch: 189, Train loss: 6672.52, Val loss: 1.09\n",
      "Epoch: 190, Train loss: 1851.69, Val loss: 2.78\n",
      "Epoch: 191, Train loss: 3338.72, Val loss: 1.09\n",
      "Epoch: 192, Train loss: 5765.62, Val loss: 1.09\n",
      "Epoch: 193, Train loss: 11118.14, Val loss: 1.09\n",
      "Epoch: 194, Train loss: 2600.90, Val loss: 1.04\n",
      "Epoch: 195, Train loss: 1.06, Val loss: 14.71\n",
      "Epoch: 196, Train loss: 4299.01, Val loss: 1.09\n",
      "Epoch: 197, Train loss: 4384.34, Val loss: 51.98\n",
      "Epoch: 198, Train loss: 6515.29, Val loss: 1.09\n",
      "Epoch: 199, Train loss: 4574.80, Val loss: 1.09\n",
      "Epoch: 200, Train loss: 952.39, Val loss: 39.75\n",
      "Epoch: 201, Train loss: 9667.19, Val loss: 1.09\n",
      "Epoch: 202, Train loss: 4757.28, Val loss: 37.44\n",
      "Epoch: 203, Train loss: 4772.05, Val loss: 1.09\n",
      "Epoch: 204, Train loss: 175.74, Val loss: 1.04\n",
      "Epoch: 205, Train loss: 596.46, Val loss: 3.10\n",
      "Epoch: 206, Train loss: 14390.70, Val loss: 6.87\n",
      "Epoch: 207, Train loss: 18383.29, Val loss: 1.09\n",
      "Epoch: 208, Train loss: 13175.70, Val loss: 1.09\n",
      "Epoch: 209, Train loss: 6867.19, Val loss: 1.09\n",
      "Epoch: 210, Train loss: 727.33, Val loss: 1.76\n",
      "Epoch: 211, Train loss: 4946.39, Val loss: 1.09\n",
      "Epoch: 212, Train loss: 1.08, Val loss: 1.09\n",
      "Epoch: 213, Train loss: 4955.02, Val loss: 1.09\n",
      "Epoch: 214, Train loss: 507.78, Val loss: 1.09\n",
      "Epoch: 215, Train loss: 2584.32, Val loss: 1.09\n",
      "Epoch: 216, Train loss: 1203.42, Val loss: 1.09\n",
      "Epoch: 217, Train loss: 1.08, Val loss: 1.09\n",
      "Epoch: 218, Train loss: 4511.45, Val loss: 1.09\n",
      "Epoch: 219, Train loss: 8377.93, Val loss: 1.09\n",
      "Epoch: 220, Train loss: 5273.20, Val loss: 6.31\n",
      "Epoch: 221, Train loss: 1.09, Val loss: 1.09\n",
      "Epoch: 222, Train loss: 2.17, Val loss: 1.09\n",
      "Epoch: 223, Train loss: 423.04, Val loss: 1.09\n",
      "Epoch: 224, Train loss: 1.09, Val loss: 1.09\n",
      "Epoch: 225, Train loss: 10541.26, Val loss: 1.09\n",
      "Epoch: 226, Train loss: 10741.97, Val loss: 1.09\n",
      "Epoch: 227, Train loss: 7458.14, Val loss: 1.04\n",
      "Epoch: 228, Train loss: 624.98, Val loss: 1.09\n",
      "Epoch: 229, Train loss: 929.50, Val loss: 1.04\n",
      "Epoch: 230, Train loss: 1.06, Val loss: 87.53\n",
      "Epoch: 231, Train loss: 12313.96, Val loss: 1.09\n",
      "Epoch: 232, Train loss: 5914.05, Val loss: 1.09\n",
      "Epoch: 233, Train loss: 1735.11, Val loss: 6.24\n",
      "Epoch: 234, Train loss: 17609.59, Val loss: 1.09\n",
      "Epoch: 235, Train loss: 1.08, Val loss: 1.09\n",
      "Epoch: 236, Train loss: 1.07, Val loss: 1.09\n",
      "Epoch: 237, Train loss: 7848.28, Val loss: 16.17\n",
      "Epoch: 238, Train loss: 1.06, Val loss: 131.34\n",
      "Epoch: 239, Train loss: 2571.47, Val loss: 3.82\n",
      "Epoch: 240, Train loss: 5135.83, Val loss: 1.09\n",
      "Epoch: 241, Train loss: 1156.34, Val loss: 12.03\n",
      "Epoch: 242, Train loss: 620.43, Val loss: 47.70\n",
      "Epoch: 243, Train loss: 1.05, Val loss: 1.09\n",
      "Epoch: 244, Train loss: 2637.71, Val loss: 1.09\n",
      "Epoch: 245, Train loss: 1.08, Val loss: 1.09\n",
      "Epoch: 246, Train loss: 4097.61, Val loss: 1.09\n",
      "Epoch: 247, Train loss: 1044.67, Val loss: 1.09\n",
      "Epoch: 248, Train loss: 852.14, Val loss: 1.09\n",
      "Epoch: 249, Train loss: 2352.12, Val loss: 1.04\n",
      "Epoch: 250, Train loss: 1.09, Val loss: 1.09\n",
      "Epoch: 251, Train loss: 7929.30, Val loss: 1.04\n",
      "Epoch: 252, Train loss: 911.45, Val loss: 6.46\n",
      "Epoch: 253, Train loss: 2503.26, Val loss: 12.08\n",
      "Epoch: 254, Train loss: 85.86, Val loss: 1.09\n",
      "Epoch: 255, Train loss: 1369.18, Val loss: 1.09\n",
      "Epoch: 256, Train loss: 66.38, Val loss: 1.09\n",
      "Epoch: 257, Train loss: 10893.38, Val loss: 1.09\n",
      "Epoch: 258, Train loss: 5806.00, Val loss: 127.60\n",
      "Epoch: 259, Train loss: 21052.09, Val loss: 1.03\n",
      "Epoch: 260, Train loss: 24889.65, Val loss: 1.09\n",
      "Epoch: 261, Train loss: 1902.07, Val loss: 1.09\n",
      "Epoch: 262, Train loss: 16503.30, Val loss: 1.09\n",
      "Epoch: 263, Train loss: 1.09, Val loss: 11.90\n",
      "Epoch: 264, Train loss: 15558.23, Val loss: 1.04\n",
      "Epoch: 265, Train loss: 1264.93, Val loss: 1.04\n",
      "Epoch: 266, Train loss: 29420.86, Val loss: 1.09\n",
      "Epoch: 267, Train loss: 153.13, Val loss: 1.09\n",
      "Epoch: 268, Train loss: 4236.46, Val loss: 37.39\n",
      "Epoch: 269, Train loss: 1911.73, Val loss: 25.23\n",
      "Epoch: 270, Train loss: 9478.60, Val loss: 1.09\n",
      "Epoch: 271, Train loss: 1.07, Val loss: 1.09\n",
      "Epoch: 272, Train loss: 4231.90, Val loss: 0.94\n",
      "Epoch: 273, Train loss: 17467.86, Val loss: 1.09\n",
      "Epoch: 274, Train loss: 5726.42, Val loss: 16.77\n",
      "Epoch: 275, Train loss: 2603.67, Val loss: 1.09\n",
      "Epoch: 276, Train loss: 446.37, Val loss: 1.04\n",
      "Epoch: 277, Train loss: 2544.66, Val loss: 1.04\n",
      "Epoch: 278, Train loss: 30172.85, Val loss: 1.09\n",
      "Epoch: 279, Train loss: 369.43, Val loss: 1.03\n",
      "Epoch: 280, Train loss: 3064.98, Val loss: 1.09\n",
      "Epoch: 281, Train loss: 103.43, Val loss: 1.09\n",
      "Epoch: 282, Train loss: 1616.74, Val loss: 1.09\n",
      "Epoch: 283, Train loss: 131.79, Val loss: 1.09\n",
      "Epoch: 284, Train loss: 1.07, Val loss: 18.76\n",
      "Epoch: 285, Train loss: 1524.92, Val loss: 1.09\n",
      "Epoch: 286, Train loss: 1122.81, Val loss: 1.09\n",
      "Epoch: 287, Train loss: 1.09, Val loss: 3.73\n",
      "Epoch: 288, Train loss: 6129.64, Val loss: 1.09\n",
      "Epoch: 289, Train loss: 124.80, Val loss: 1.04\n",
      "Epoch: 290, Train loss: 93.58, Val loss: 1.09\n",
      "Epoch: 291, Train loss: 3251.41, Val loss: 1.04\n",
      "Epoch: 292, Train loss: 1438.53, Val loss: 1.09\n",
      "Epoch: 293, Train loss: 5581.52, Val loss: 1.09\n",
      "Epoch: 294, Train loss: 1.05, Val loss: 6.59\n",
      "Epoch: 295, Train loss: 1.09, Val loss: 1.09\n",
      "Epoch: 296, Train loss: 462.49, Val loss: 1.03\n",
      "Epoch: 297, Train loss: 1282.99, Val loss: 34.02\n",
      "Epoch: 298, Train loss: 587.39, Val loss: 1.09\n",
      "Epoch: 299, Train loss: 1.06, Val loss: 1.36\n",
      "Epoch: 300, Train loss: 1901.23, Val loss: 1.09\n",
      "Epoch: 301, Train loss: 653.02, Val loss: 1.09\n",
      "Epoch: 302, Train loss: 1.07, Val loss: 1.09\n",
      "Epoch: 303, Train loss: 3443.25, Val loss: 1.09\n",
      "Epoch: 304, Train loss: 7833.43, Val loss: 1.03\n",
      "Epoch: 305, Train loss: 2432.37, Val loss: 176.53\n",
      "Epoch: 306, Train loss: 3324.67, Val loss: 79.35\n",
      "Epoch: 307, Train loss: 1.08, Val loss: 119.61\n",
      "Epoch: 308, Train loss: 296.89, Val loss: 1.09\n",
      "Epoch: 309, Train loss: 3135.38, Val loss: 97.29\n",
      "Epoch: 310, Train loss: 1932.64, Val loss: 1.09\n",
      "Epoch: 311, Train loss: 1507.02, Val loss: 1.04\n",
      "Epoch: 312, Train loss: 2240.38, Val loss: 1.03\n",
      "Epoch: 313, Train loss: 1.09, Val loss: 1.09\n",
      "Epoch: 314, Train loss: 11.27, Val loss: 37.85\n",
      "Epoch: 315, Train loss: 3127.86, Val loss: 1.09\n",
      "Epoch: 316, Train loss: 1.09, Val loss: 1.03\n",
      "Epoch: 317, Train loss: 2231.43, Val loss: 1.09\n",
      "Epoch: 318, Train loss: 1.09, Val loss: 1.09\n",
      "Epoch: 319, Train loss: 1.09, Val loss: 1.09\n",
      "Epoch: 320, Train loss: 2176.37, Val loss: 1.09\n",
      "Epoch: 321, Train loss: 10926.83, Val loss: 1.09\n",
      "Epoch: 322, Train loss: 1.07, Val loss: 1.09\n",
      "Epoch: 323, Train loss: 1.09, Val loss: 1.09\n",
      "Epoch: 324, Train loss: 74.57, Val loss: 156.31\n",
      "Epoch: 325, Train loss: 1.09, Val loss: 1.09\n",
      "Epoch: 326, Train loss: 1.07, Val loss: 1.09\n",
      "Epoch: 327, Train loss: 6886.02, Val loss: 91.78\n",
      "Epoch: 328, Train loss: 1.09, Val loss: 9.49\n",
      "Epoch: 329, Train loss: 1.09, Val loss: 1.09\n",
      "Epoch: 330, Train loss: 8137.24, Val loss: 1.09\n",
      "Epoch: 331, Train loss: 1.07, Val loss: 1.09\n",
      "Epoch: 332, Train loss: 116.73, Val loss: 1.09\n",
      "Epoch: 333, Train loss: 1344.59, Val loss: 18.42\n",
      "Epoch: 334, Train loss: 1.07, Val loss: 1.09\n",
      "Epoch: 335, Train loss: 3093.10, Val loss: 1.09\n",
      "Epoch: 336, Train loss: 1303.73, Val loss: 1.09\n",
      "Epoch: 337, Train loss: 1277.87, Val loss: 1.09\n",
      "Epoch: 338, Train loss: 567.62, Val loss: 1.09\n",
      "Epoch: 339, Train loss: 1.09, Val loss: 1.09\n",
      "Epoch: 340, Train loss: 2264.68, Val loss: 1.03\n",
      "Epoch: 341, Train loss: 1.07, Val loss: 1.09\n",
      "Epoch: 342, Train loss: 1.09, Val loss: 1.09\n",
      "Epoch: 343, Train loss: 2935.93, Val loss: 1.09\n",
      "Epoch: 344, Train loss: 4587.92, Val loss: 1.09\n",
      "Epoch: 345, Train loss: 5641.10, Val loss: 1.09\n",
      "Epoch: 346, Train loss: 1.08, Val loss: 1.09\n",
      "Epoch: 347, Train loss: 1.08, Val loss: 1.09\n",
      "Epoch: 348, Train loss: 2998.57, Val loss: 1.09\n",
      "Epoch: 349, Train loss: 1.07, Val loss: 1.09\n",
      "Epoch: 350, Train loss: 96.78, Val loss: 1.09\n",
      "Epoch: 351, Train loss: 96.30, Val loss: 1.09\n",
      "Epoch: 352, Train loss: 1.08, Val loss: 1.09\n",
      "Epoch: 353, Train loss: 6480.02, Val loss: 195.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 354, Train loss: 1.08, Val loss: 1.09\n",
      "Epoch: 355, Train loss: 1.08, Val loss: 1.09\n",
      "Epoch: 356, Train loss: 1.08, Val loss: 1.09\n",
      "Epoch: 357, Train loss: 2295.06, Val loss: 1.09\n",
      "Epoch: 358, Train loss: 11909.06, Val loss: 11.71\n",
      "Epoch: 359, Train loss: 11.83, Val loss: 1.09\n",
      "Epoch: 360, Train loss: 1392.76, Val loss: 1.09\n",
      "Epoch: 361, Train loss: 1.07, Val loss: 151.40\n",
      "Epoch: 362, Train loss: 4346.39, Val loss: 1.09\n",
      "Epoch: 363, Train loss: 739.55, Val loss: 1.09\n",
      "Epoch: 364, Train loss: 2387.11, Val loss: 1.09\n",
      "Epoch: 365, Train loss: 897.76, Val loss: 1.09\n",
      "Epoch: 366, Train loss: 309.24, Val loss: 1.09\n",
      "Epoch: 367, Train loss: 1.08, Val loss: 1.09\n",
      "Epoch: 368, Train loss: 1.08, Val loss: 1.09\n",
      "Epoch: 369, Train loss: 1.08, Val loss: 1.09\n",
      "Epoch: 370, Train loss: 1.08, Val loss: 1.09\n",
      "Epoch: 371, Train loss: 1.06, Val loss: 26.91\n",
      "Epoch: 372, Train loss: 5914.45, Val loss: 1.08\n",
      "Epoch: 373, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 374, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 375, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 376, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 377, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 378, Train loss: 573.84, Val loss: 1.08\n",
      "Epoch: 379, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 380, Train loss: 1426.47, Val loss: 1.08\n",
      "Epoch: 381, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 382, Train loss: 646.43, Val loss: 1.04\n",
      "Epoch: 383, Train loss: 6132.53, Val loss: 2.40\n",
      "Epoch: 384, Train loss: 6085.50, Val loss: 1.08\n",
      "Epoch: 385, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 386, Train loss: 747.26, Val loss: 1.04\n",
      "Epoch: 387, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 388, Train loss: 1.08, Val loss: 111.89\n",
      "Epoch: 389, Train loss: 1999.60, Val loss: 1.08\n",
      "Epoch: 390, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 391, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 392, Train loss: 1.07, Val loss: 1.08\n",
      "Epoch: 393, Train loss: 1444.79, Val loss: 1.08\n",
      "Epoch: 394, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 395, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 396, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 397, Train loss: 1.06, Val loss: 3.36\n",
      "Epoch: 398, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 399, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 400, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 401, Train loss: 3775.82, Val loss: 1.08\n",
      "Epoch: 402, Train loss: 1.07, Val loss: 1.08\n",
      "Epoch: 403, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 404, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 405, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 406, Train loss: 1047.71, Val loss: 1.08\n",
      "Epoch: 407, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 408, Train loss: 1.07, Val loss: 1.08\n",
      "Epoch: 409, Train loss: 1.07, Val loss: 1.08\n",
      "Epoch: 410, Train loss: 2628.33, Val loss: 1.08\n",
      "Epoch: 411, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 412, Train loss: 1458.58, Val loss: 1.08\n",
      "Epoch: 413, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 414, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 415, Train loss: 103.10, Val loss: 92.94\n",
      "Epoch: 416, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 417, Train loss: 134.80, Val loss: 1.08\n",
      "Epoch: 418, Train loss: 986.05, Val loss: 1.08\n",
      "Epoch: 419, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 420, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 421, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 422, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 423, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 424, Train loss: 471.60, Val loss: 1.08\n",
      "Epoch: 425, Train loss: 129.56, Val loss: 1.03\n",
      "Epoch: 426, Train loss: 1.07, Val loss: 1.08\n",
      "Epoch: 427, Train loss: 1.08, Val loss: 1.03\n",
      "Epoch: 428, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 429, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 430, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 431, Train loss: 1.07, Val loss: 1.08\n",
      "Epoch: 432, Train loss: 209.44, Val loss: 1.08\n",
      "Epoch: 433, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 434, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 435, Train loss: 1620.00, Val loss: 1.08\n",
      "Epoch: 436, Train loss: 1.07, Val loss: 2.99\n",
      "Epoch: 437, Train loss: 1.07, Val loss: 1.08\n",
      "Epoch: 438, Train loss: 1175.28, Val loss: 1.08\n",
      "Epoch: 439, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 440, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 441, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 442, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 443, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 444, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 445, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 446, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 447, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 448, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 449, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 450, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 451, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 452, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 453, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 454, Train loss: 1.07, Val loss: 1.08\n",
      "Epoch: 455, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 456, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 457, Train loss: 1.07, Val loss: 1.08\n",
      "Epoch: 458, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 459, Train loss: 3733.32, Val loss: 1.08\n",
      "Epoch: 460, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 461, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 462, Train loss: 1.07, Val loss: 1.08\n",
      "Epoch: 463, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 464, Train loss: 183.37, Val loss: 3.64\n",
      "Epoch: 465, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 466, Train loss: 565.11, Val loss: 1.08\n",
      "Epoch: 467, Train loss: 1.07, Val loss: 1.08\n",
      "Epoch: 468, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 469, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 470, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 471, Train loss: 48.41, Val loss: 1.08\n",
      "Epoch: 472, Train loss: 762.80, Val loss: 1.08\n",
      "Epoch: 473, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 474, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 475, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 476, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 477, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 478, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 479, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 480, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 481, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 482, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 483, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 484, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 485, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 486, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 487, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 488, Train loss: 4172.69, Val loss: 1.08\n",
      "Epoch: 489, Train loss: 389.23, Val loss: 1.08\n",
      "Epoch: 490, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 491, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 492, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 493, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 494, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 495, Train loss: 1.08, Val loss: 1.08\n",
      "Epoch: 496, Train loss: 679.15, Val loss: 1.08\n",
      "Epoch: 497, Train loss: 679.29, Val loss: 1.08\n",
      "Epoch: 498, Train loss: 138.66, Val loss: 7.74\n",
      "Epoch: 499, Train loss: 1.08, Val loss: 1.03\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for i in range(1,2):\n",
    "    train_data = MyOwnDataset(root=f'./data/folds/training_folds/fold{i}', pre_transform=None, transform=None, fold=i)\n",
    "    val_data = MyOwnDataset(root=f'./data/folds/validation_folds/fold{i}', pre_transform=None, transform=None, fold=i)\n",
    "    train_loader = DataLoader(train_data, batch_size=1, shuffle = True)\n",
    "    val_loader = DataLoader(val_data, batch_size=1, shuffle = True)\n",
    "    \n",
    "    num_node, num_feature, num_edge = (train_data[0].x).shape[0], (train_data[0].x).shape[1], (train_data[0].edge_index).shape[0]\n",
    "    model = GINTopK(num_feature=num_feature, num_class=num_class, nhid=nhid).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
    "\n",
    "    def train(data):\n",
    "        optimizer.zero_grad()\n",
    "        out, h = model(data)\n",
    "        loss = F.nll_loss(out, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss, h\n",
    "    \n",
    "    def test(data):\n",
    "        out, h = model(data)\n",
    "        val_loss = F.nll_loss(out, data.y.view(-1))\n",
    "        return val_loss, h\n",
    "\n",
    "    run=1\n",
    "    for epoch in range(epochs):\n",
    "        data_train = (train_data[0]).to(device)\n",
    "        data_val = (val_data[0]).to(device)\n",
    "        t_loss, h = train(data_train)\n",
    "        v_loss, h = test(data_val)\n",
    "        train_loss.append((t_loss.cpu()).detach().numpy())\n",
    "        val_loss.append((v_loss.cpu()).detach().numpy())\n",
    "        print(\"Epoch: {}, Train loss: {:.2f}, Val loss: {:.2f}\".format(epoch,t_loss,v_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c29ecd95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the loss as the network trained\n",
    "# for for run in range(runs):\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "train_loss_percent = [i / 100 for i in train_loss]\n",
    "val_loss_percent = [i / 100 for i in val_loss]\n",
    "# print(len(val_loss_percent))\n",
    "# print(val_loss_percent)\n",
    "\n",
    "plt.plot(train_loss_percent, label='Training Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(val_loss_percent,label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407aeba7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
